{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## DS 5460 Big Data Scaling, SPRING 2024\n\n##      Homework Assignment \\#1: Introduction to MapReduce\n\nObjectives:\n- This assignment is designed to help students better understand the MapReduce Paradigm through hands-on practice.\n- Students will become familiar (if not already) with Linux/Unix piping and sorting and Jupyter magic command `%%writefile` and `%%timeit`. \n\nInstructions:\nThis assignment consists of 6 questions. Please be sure to read all text cell descriptions and comments closely to fill in your solution when expected. We will only grade code written in the designated spaces. If a question's instructions are unclear, please reach out for clarification on Piazza. We expect each student to write their own code independently. If GenAI tools are used, **where and how they were used must be disclosed properly in a separate text cell at the end of this notebook**.\n\n__TIPS:__ \n1. If you're not familiar with Linux/Unix **piping** and **redirecting**, check out this tutorial first: https://ryanstutorials.net/linuxtutorial/piping.php You will need to understand the differences to answer some of the later questions.\n2. Make use of your peers and TAs by asking questions on Piazza. Everyone has different experiences and background so don't be shy; all questions are welcome!\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Notebook Set-Up\nBefore starting your homework run the following cells to confirm your setup."}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "!mkdir -p /notebooks/hw1 "}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "/notebooks/hw1\n"}], "source": "# TODO: update the working directory with the location of your homework notebook\n%cd /notebooks/hw1 "}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "/notebooks/hw1\n"}], "source": "!pwd"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"data": {"text/plain": "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "# confirm you are running Python 3\nimport sys\nsys.version_info"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "# imports\nimport re\nimport sys"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 1: Download the Data\n\nIn this assignment we'll continue to work with the free plain text version of _Alice's Adventures in Wonderland_ available from Project Gutenberg. __Use the first two cells below to download this text from http://www.gutenberg.org/files/11/11-0.txt and preview the first few lines.__ "}, {"cell_type": "code", "execution_count": 34, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  170k  100  170k    0     0   579k      0 --:--:-- --:--:-- --:--:--  579k\n"}], "source": "# TODO: write a Unix command download the data to this local file /data/alice.txt (like we did in week2-demo)\n# from http://www.gutenberg.org/files/11/11-0.txt \n!curl -L http://www.gutenberg.org/files/11/11-0.txt --output /data/alice.txt"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\ufeffThe Project Gutenberg eBook of Alice\u2019s Adventures in Wonderland, by Lewis Carroll\n\nThis eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\n"}], "source": "# TODO: write a Unix command to print the first 5 lines of your downloaded file\n!head -n 5 /data/alice.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "It's nice to develop a habit of creating small files with simulated data for use in developing, debugging and testing your code, particularly in this assignment. The jupyter magic command `%%writefile` is a convenient way to do this. __Run the following cells to create a test data folder and file for use in our word counting task.__"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing /data/hw1_test.txt\n"}], "source": "%%writefile /data/hw1_test.txt\nGood luck on the first homework assignment.\nHope you have fun while practicing your Linux/Unix skills"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "alice_test.txt\nhw1_test.txt\n"}], "source": "# confirm the file was created in the data directory using a grep command:\n!ls /data | grep test"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 2: Word Count in Python\n\nIn this part of the homework, you will practice writing Python scripts to read from standard input and using Unix piping commands to run these programs and save their output to file, which are particularly useful skills to have when developing programs in the cloud. __In this question you will write a short python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a 'baseline' implementation that we'll later compare to the parallelized version of the same task.\n\n### Q2 Tasks:\n\n* __a) code:__ Complete the Python script in the file __`wordCount.py`__. Read all comments carefully to be sure you understand the expected behavior of this function. Please do not code outside of the marked location.\n\n\n* __b) testing:__ Run the cell marked `part b` to call your script on the test file we created above (`data/hw1_test.txt`). Confirm that your script returns the correct counts for each word by visually comparing the output to the test file. \n\n\n* __c) results:__ When you are confident in your implementation, run the cell marked `part c` to count the number of occurrences of each word in _Alice's Adventures in Wonderland_. In the same cell we'll pipe the output to file. Then use the provided `grep` commands to check your answers.\n\n\n* __d) short response:__ Suppose you decide that you'd really like a word and its plural (e.g. 'hatter' and 'hatters' or 'person' and 'people') to be counted as the same word. After we have run the wordcount would it be more efficient to post-process your output file or discard your output file and start the analysis over with a new tokenizer? Briefly explain your reasoning."}, {"cell_type": "markdown", "metadata": {}, "source": "### Q2 Student Answers:\n> __a-c)__ _Complete the coding portions of this question before answering 'd'._\n\n> __d)__ TODO: Type your answer here!"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part a - DO YOUR WORK IN wordCount.py"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "!chmod a+x /wordCount.py"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "#!/usr/bin/env python\n\"\"\"\nThis script reads lines from STDIN and returns a list of\nall words and the count of how many times they occurred.\n\nINPUT:\n    a text file\nOUTPUT FORMAT:\n    word \\t count\nUSAGE:\n    python wordCount.py < yourTextFile.txt\n\nInstructions:\n    Fill in the missing code below so that the script\n    prints tab (\\t) separated word counts to Standard Output.\n    NOTE: the tokenizing is already done for you, please do\n    NOT modify the provided code or you risk breaking things.\n\"\"\"\n\n# imports\nimport sys\nimport re\nfrom collections import defaultdict\n\ncounts = defaultdict(int)\n\n# stream over lines from Standard Input\nfor line in sys.stdin:\n\n    # tokenize\n    line = line.strip()\n    words = re.findall(r'[a-z]+', line.lower())\n\n############ TODO: YOUR CODE HERE #########\n    for word in words:\n        counts[word] += 1\n\nfor word, count in counts.items():\n    print(f'{word}\\t{count}')\n\n\n############ (END) YOUR CODE ##############\n"}], "source": "# part a - DO NOT MODIFY THIS CELL - run this cell after you \n# complete wordCount.py so the TA can view your work\n!cat /wordCount.py"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "good\t1\nluck\t1\non\t1\nthe\t1\nfirst\t1\nhomework\t1\nassignment\t1\nhope\t1\nyou\t1\nhave\t1\nfun\t1\nwhile\t1\npracticing\t1\nyour\t1\nlinux\t1\nunix\t1\nskills\t1\n"}], "source": "# part b - DO NOT MODIFY THIS CELL, just run it as is to test your script\n!python /wordCount.py < /data/hw1_test.txt"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": "# part c - DO NOT MODIFY THIS CELL, just run it as is to perform the word count.\n!python /wordCount.py < /data/alice.txt > /data/alice_counts.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "Take a look at the first 10 words & their counts."}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "the\t1839\nproject\t88\ngutenberg\t98\nebook\t13\nof\t638\nalice\t403\ns\t222\nadventures\t11\nin\t435\nwonderland\t7\n"}], "source": "!head /data/alice_counts.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "__Check your results:__ How many times does the word \"alice\" appear in the book? "}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "alice\t403\n"}], "source": "# EXPECTED OUTPUT: 403\n!grep alice /data/alice_counts.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "__Check your results:__ How many times does the word \"hatter\" appear in the book? "}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "hatter\t56\nhatters\t1\n"}], "source": "# EXPECTED OUTPUT: 56\n!grep hatter /data/alice_counts.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "__Check your results:__ How many times does the word \"queen\" appear in the book? "}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "queen\t76\nqueens\t1\n"}], "source": "# EXPECTED OUTPUT: 76\n!grep queen /data/alice_counts.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 3: Unix Sorting Practice\nSorting is a useful tool and an important process in MapReduce. Let's practice using Linux/Unix sort.\n\n### Q3 Tasks:\n*   0) code: Write a Unix Command to check how many records are in your word count file.\n* __a) code:__ Write a Unix command to sort your word count file alphabetically. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) the results to `data/alice_counts_A-Z.txt`. [*HINT: if Unix sort commands are new to you, start with [this biowize blogpost](https://biowize.wordpress.com/2015/03/13/unix-sort-sorting-with-both-numeric-and-non-numeric-keys/) or [this unixschool tutorial](http://www.theunixschool.com/2012/08/linux-sort-command-examples.html)*]\n\n* __b) code:__ Write a Unix command to sort your word count file from highest to lowest count. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) your results to `data/alice_counts_sorted.txt`; then run the provided cell to print the top ten words. Compare your output to the expected output provided."}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3006 /data/alice_counts.txt\n"}], "source": "# part 0: write a command to check how many records are in your word count file (data/alice_count.txt)\n!wc -l /data/alice_counts.txt"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": "# part a - unix command to sort your word counts alphabetically \n!cat /data/alice_counts.txt | sort -k1,1 > /data/alice_counts_A-Z.txt"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\t695\nabide\t2\nable\t1\nabout\t102\nabove\t3\nabsence\t1\nabsurd\t2\naccept\t1\nacceptance\t1\naccepted\t2\n"}], "source": "# part a - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n!head /data/alice_counts_A-Z.txt"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": "# part b - unix command to sort your word counts from highest to lowest count\n!cat /data/alice_counts.txt | sort -k2,2nr -k1,1 > /data/alice_counts_sorted.txt"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "the\t1839\nand\t942\nto\t811\na\t695\nof\t638\nit\t610\nshe\t553\ni\t546\nyou\t486\nsaid\t462\n"}], "source": "# part b - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n!head /data/alice_counts_sorted.txt  "}, {"cell_type": "markdown", "metadata": {}, "source": "<table>\n<th>expected output for (a):</th>\n<th>expected output for (b):</th>\n<tr><td><pre>\na\t695\nabide\t2\nable\t1\nabout\t102\nabove\t3\nabsence\t1\nabsurd\t2\naccept\t1\nacceptance\t1\naccepted\t2\n</pre></td>\n<td><pre>\nthe\t1839\nand\t942\nto\t811\na\t695\nof\t638\nit\t610\nshe\t553\ni\t546\nyou\t486\nsaid\t462\n</pre></td></tr>\n</table>"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 4: Simplified Parallel Word Count \nInstead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. __In this question you'll use a bash script to \"parallelize\" your Word Count.__\n\n\n### Q4 Tasks:\n* __a) run provided script:__ The bash script `parallel_wc_v1.sh` takes an input file, splits it into a specified number of 'chunks', and then applies a script of your choice to each chunk. Read through the comments (you are also welcome to read the contents) of this bash file to understand how to run this script - similar to reading any other documentation, pay close attention to the arguments expected. In part a), complete the command to use this script to apply `wordCount.py` to the _Alice_ text in **FOUR (4)** parallel processes. Redirect the results into a file called `alice_pCounts.txt.`\n\n\n* __b) short response:__ Examine the output from part a) and explain if the output matched what you expected and why or why not.\n\n\n* __c) run provided script:__ Another script `aggregateCounts.py` reads word counts from standard input and combines any duplicates it encounters. Read through this script to be sure you understand what it does. Then follow the instructions in `parallel_wc_v2.sh` to complete the command in part c) that accepts `aggregateCounts.py` as a 4th argument.  Similarly, run with **FOUR (4)** parallel processes. Redirect the results into a file called `alice_pCounts_v2.txt.` Run the cell below to confirm that you now get the correct results for your 'alice' count. \n\n* __d) short response:__ Does the order of your scripts passed into the command matter? Explain why or why not."}, {"cell_type": "markdown", "metadata": {}, "source": "### Q4 Student Answers:\n> __b)__ The output matches my expectation as there is no reducer applied in the mapper.\n\n> __d)__ Yes, because reducer=$4 which means reducer must be on the fourth argument passed to the shell script."}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": "# part a - make sure your scripts are executable (RUN THIS CELL AS IS)\n!chmod a+x /parallel_wc_v1.sh\n!chmod a+x /wordCount.py"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": "# part a - TODO: copmlete the command to run 4 parallel processes based on the usage of the parallel_wc_v1.sh script\n!/parallel_wc_v1.sh 4 /data/alice.txt /wordCount.py  > /data/alice_pCounts.txt"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "alice\t113\nalice\t126\nalice\t122\nalice\t42\n"}], "source": "# part b - check alice count (RUN THIS CELL AS IS)\n!grep alice /data/alice_pCounts.txt"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": "# part c - make sure the aggregateCounts script is executable and parallel_wc_v2 script is executable (RUN THIS CELL AS IS)\n!chmod a+x /parallel_wc_v2.sh \n!chmod a+x /aggregateCounts.py"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [], "source": "# part c - TODO: copmlete the command to run 4 parallel processes and apply the aggregateCounts.py script\n!/parallel_wc_v2.sh 4 /data/alice.txt /wordCount.py /aggregateCounts.py > /data/alice_pCounts_v2.txt"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "alice\t403\n"}], "source": "# part c - check alice count (RUN THIS CELL AS IS)\n!grep alice /data/alice_pCounts_v2.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 5: Hadoop Streaming for Average Temperature Calculation\n\nIn this question, you will apply Hadoop Streaming to calculate the average temperature per city from the provided dataset named `temperature_data.csv`. _Note: you will need to upload the data to your cluster's local storage_. This will involve writing and using mapper and reducer scripts in Python, and then executing a Hadoop Streaming job.\n\n\n### Q5 Tasks:\n* __a) mapper and reducer scripts development:__ Write two Python scripts, `mapper_temp.py` and `reducer_temp.py`. The mapper script should process each line of the input dataset, which contains city names and corresponding temperatures, and output the city name and temperature. The reducer script should read the mapper output, calculate the average temperature for each city, and output the city name with its average temperature.\n\n* __b) unit testing:__ Make sure you unit test the running of your mapper and reducer scripts using a small dataset like we did before with `alice_test.txt` or `hw1_test.txt`. This time, you will want to create a small data file. _Challenge: see if you can use Unix commands to output the first 5 lines of the provided dataset to a new test file!_\n\n* __c) run a Hadoop Streaming job:__ Use the Hadoop Streaming command to execute your MapReduce job. The command should include the paths to your mapper and reducer scripts, the input file path to `temperature_data.csv`, and the output directory path. Ensure that you properly configure all necessary arguments and environmental variables (Feel free to add additional cells before the hadoop streaming command to do this). After running the job, inspect the output to verify correctness. \n\n* __d) explore Hadoop Streaming processes:__ Modify your Hadoop Streaming command to experiment with different numbers of reducers and observe how this affects the execution time and the output. In your response, describe how changing the number of reducers impacts the MapReduce job's performance and result organization. _Hint: You will find the `%%timeit` magic command very helpful._\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Q5 Student Answers:\n\n> __d)__ The time increases as the number of reducers increases.\n\nnumber of reducers = 1: 33 s \u00b1 15.2 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nnumber of reducers = 2: 43 s \u00b1 28.9 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nnumber of reducers = 3: 45.8 s \u00b1 500 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part a - Do your work in mapper_temp.py and reducer_temp.py"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part b - TODO: show your unit test here"}, {"cell_type": "code", "execution_count": 104, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Overwriting /hw1/tem_test.csv\n"}], "source": "%%writefile /hw1/tem_test.csv\nSan Antonio,34.8\nSan Antonio,21.2\nSan Antonio,28.0\nSan Jose,7.8\nPhoenix,26.7\nPhoenix,24.4\nPhoenix,32.9\nDallas,16.3\nDallas,5.2\nDallas,6.4"}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [], "source": "!chmod a+x /mapper_temp.py /reducer_temp.py"}, {"cell_type": "code", "execution_count": 111, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "San Antonio\t34.8\nSan Antonio\t21.2\nSan Antonio\t28.0\nSan Jose\t7.8\nPhoenix\t26.7\nPhoenix\t24.4\nPhoenix\t32.9\nDallas\t16.3\nDallas\t5.2\nDallas\t6.4\n"}], "source": "!cat /hw1/tem_test.csv | /mapper_temp.py"}, {"cell_type": "code", "execution_count": 117, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "San Antonio\t28.0\nSan Jose\t7.8\nPhoenix\t28.0\nDallas\t9.299999999999999\n"}], "source": "!echo 'San Antonio\t34.8\\nSan Antonio\t21.2\\nSan Antonio\t28.0\\nSan Jose\t7.8\\nPhoenix\t26.7\\nPhoenix\t24.4\\nPhoenix\t32.9\\nDallas\t16.3\\nDallas\t5.2\\nDallas\t6.4' | /reducer_temp.py"}, {"cell_type": "code", "execution_count": 123, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "rm: `/user/root/hw1/temperature_data.csv': No such file or directory\n"}, {"data": {"text/plain": "'/opt/conda/anaconda/bin:/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'"}, "execution_count": 123, "metadata": {}, "output_type": "execute_result"}], "source": "# part c - TODO: write the Hadoop Streaming command to test out your mapper and reducer scripts on the temperature dataset\n\nJAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\nHDFS_DIR = \"/user/root/hw1\"\n!hdfs dfs -mkdir -p {HDFS_DIR}\n!hdfs dfs -rm -r {HDFS_DIR}/temperature_data.csv\n!hdfs dfs -put /hw1/temperature_data.csv {HDFS_DIR}\n# store notebook environment path\nfrom os import environ\nPATH = environ['PATH']\nPATH"}, {"cell_type": "code", "execution_count": 124, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "rm: `/user/root/hw1/test-output': No such file or directory\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob5002272313849714812.jar tmpDir=null\n24/01/30 02:17:30 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 02:17:30 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 02:17:30 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 02:17:30 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 02:17:30 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 02:17:31 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 02:17:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0009\n24/01/30 02:17:31 INFO conf.Configuration: resource-types.xml not found\n24/01/30 02:17:31 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 02:17:31 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 02:17:31 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 02:17:31 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0009\n24/01/30 02:17:31 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0009/\n24/01/30 02:17:31 INFO mapreduce.Job: Running job: job_1706529591615_0009\n24/01/30 02:17:38 INFO mapreduce.Job: Job job_1706529591615_0009 running in uber mode : false\n24/01/30 02:17:38 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 02:17:46 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 02:17:52 INFO mapreduce.Job:  map 56% reduce 0%\n24/01/30 02:17:53 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 02:17:58 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 02:17:59 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 02:18:04 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 02:18:07 INFO mapreduce.Job: Job job_1706529591615_0009 completed successfully\n24/01/30 02:18:07 INFO mapreduce.Job: Counters: 49\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=153906\n\t\tTotal time spent by all reduces in occupied slots (ms)=8568\n\t\tTotal time spent by all map tasks (ms)=51302\n\t\tTotal time spent by all reduce tasks (ms)=2856\n\t\tTotal vcore-milliseconds taken by all map tasks=51302\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2856\n\t\tTotal megabyte-milliseconds taken by all map tasks=157599744\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8773632\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1338\n\t\tCPU time spent (ms)=17930\n\t\tPhysical memory (bytes) snapshot=5420974080\n\t\tVirtual memory (bytes) snapshot=44274642944\n\t\tTotal committed heap usage (bytes)=4845469696\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 02:18:07 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\n"}], "source": "!hdfs dfs -rm -r {HDFS_DIR}/test-output\n\n!hadoop jar {JAR_FILE} \\\n  -D stream.num.map.output.key.fields=1 \\\n  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n  -files /mapper_temp.py,/reducer_temp.py \\\n  -mapper mapper_temp.py \\\n  -reducer reducer_temp.py \\\n  -input {HDFS_DIR}/temperature_data.csv \\\n  -output {HDFS_DIR}/test-output \\\n  -numReduceTasks 1 \\\n  -cmdenv PATH={PATH}"}, {"cell_type": "code", "execution_count": 125, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\n-rw-r--r--   1 root hadoop          0 2024-01-30 02:18 /user/root/hw1/test-output/_SUCCESS\n-rw-r--r--   1 root hadoop        284 2024-01-30 02:18 /user/root/hw1/test-output/part-00000\n"}], "source": "!hdfs dfs -ls {HDFS_DIR}/test-output"}, {"cell_type": "code", "execution_count": 126, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Chicago\t7.4626437921459665\nDallas\t17.57430145217826\nHouston\t19.935502724120877\nLos Angeles\t22.517839195979853\nNew York\t12.611734492295556\nPhiladelphia\t14.906933574152164\nPhoenix\t27.571388159224064\nSan Antonio\t22.48523624013575\nSan Diego\t20.008504635227766\nSan Jose\t17.493455786409804\n"}], "source": "!hdfs dfs -cat {HDFS_DIR}/test-output/part-0000* > /results_tem.txt\n!head -n 20 /results_tem.txt"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part d - TODO: your explorations here with number of reducers"}, {"cell_type": "code", "execution_count": 127, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob2952226784069472598.jar tmpDir=null\n24/01/30 03:11:17 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:11:17 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:11:18 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:11:18 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:11:18 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:11:18 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:11:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0010\n24/01/30 03:11:18 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:11:18 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:11:18 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:11:18 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:11:18 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0010\n24/01/30 03:11:18 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0010/\n24/01/30 03:11:18 INFO mapreduce.Job: Running job: job_1706529591615_0010\n24/01/30 03:11:25 INFO mapreduce.Job: Job job_1706529591615_0010 running in uber mode : false\n24/01/30 03:11:25 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:11:33 INFO mapreduce.Job:  map 11% reduce 0%\n24/01/30 03:11:34 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:11:40 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:11:47 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:11:52 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:11:54 INFO mapreduce.Job: Job job_1706529591615_0010 completed successfully\n24/01/30 03:11:54 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=159906\n\t\tTotal time spent by all reduces in occupied slots (ms)=8373\n\t\tTotal time spent by all map tasks (ms)=53302\n\t\tTotal time spent by all reduce tasks (ms)=2791\n\t\tTotal vcore-milliseconds taken by all map tasks=53302\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2791\n\t\tTotal megabyte-milliseconds taken by all map tasks=163743744\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8573952\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1331\n\t\tCPU time spent (ms)=17550\n\t\tPhysical memory (bytes) snapshot=5478637568\n\t\tVirtual memory (bytes) snapshot=44449431552\n\t\tTotal committed heap usage (bytes)=4958715904\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:11:54 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7449522084988705745.jar tmpDir=null\n24/01/30 03:11:58 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:11:58 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:11:58 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:11:58 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:11:59 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:11:59 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:11:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0011\n24/01/30 03:11:59 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:11:59 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:11:59 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:11:59 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:11:59 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0011\n24/01/30 03:11:59 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0011/\n24/01/30 03:11:59 INFO mapreduce.Job: Running job: job_1706529591615_0011\n24/01/30 03:12:07 INFO mapreduce.Job: Job job_1706529591615_0011 running in uber mode : false\n24/01/30 03:12:07 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:12:14 INFO mapreduce.Job:  map 11% reduce 0%\n24/01/30 03:12:15 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:12:21 INFO mapreduce.Job:  map 56% reduce 0%\n24/01/30 03:12:22 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:12:28 INFO mapreduce.Job:  map 89% reduce 0%\n24/01/30 03:12:29 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:12:34 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:12:37 INFO mapreduce.Job: Job job_1706529591615_0011 completed successfully\n24/01/30 03:12:37 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=159015\n\t\tTotal time spent by all reduces in occupied slots (ms)=8409\n\t\tTotal time spent by all map tasks (ms)=53005\n\t\tTotal time spent by all reduce tasks (ms)=2803\n\t\tTotal vcore-milliseconds taken by all map tasks=53005\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2803\n\t\tTotal megabyte-milliseconds taken by all map tasks=162831360\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8610816\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1414\n\t\tCPU time spent (ms)=17540\n\t\tPhysical memory (bytes) snapshot=5306572800\n\t\tVirtual memory (bytes) snapshot=44255784960\n\t\tTotal committed heap usage (bytes)=4727504896\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:12:37 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob6510961388316527613.jar tmpDir=null\n24/01/30 03:12:41 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:12:41 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:12:41 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:12:41 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:12:42 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:12:42 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:12:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0012\n24/01/30 03:12:43 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:12:43 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:12:43 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:12:43 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:12:43 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0012\n24/01/30 03:12:43 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0012/\n24/01/30 03:12:43 INFO mapreduce.Job: Running job: job_1706529591615_0012\n24/01/30 03:12:50 INFO mapreduce.Job: Job job_1706529591615_0012 running in uber mode : false\n24/01/30 03:12:50 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:12:57 INFO mapreduce.Job:  map 11% reduce 0%\n24/01/30 03:12:58 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:13:04 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:13:12 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:13:17 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:13:19 INFO mapreduce.Job: Job job_1706529591615_0012 completed successfully\n24/01/30 03:13:19 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=162693\n\t\tTotal time spent by all reduces in occupied slots (ms)=8325\n\t\tTotal time spent by all map tasks (ms)=54231\n\t\tTotal time spent by all reduce tasks (ms)=2775\n\t\tTotal vcore-milliseconds taken by all map tasks=54231\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2775\n\t\tTotal megabyte-milliseconds taken by all map tasks=166597632\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8524800\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1341\n\t\tCPU time spent (ms)=18150\n\t\tPhysical memory (bytes) snapshot=5435994112\n\t\tVirtual memory (bytes) snapshot=44357505024\n\t\tTotal committed heap usage (bytes)=4842848256\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:13:19 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob2515249973126692070.jar tmpDir=null\n24/01/30 03:13:23 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:13:23 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:13:24 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:13:24 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:13:24 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:13:24 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:13:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0013\n24/01/30 03:13:25 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:13:25 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:13:25 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:13:25 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:13:25 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0013\n24/01/30 03:13:25 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0013/\n24/01/30 03:13:25 INFO mapreduce.Job: Running job: job_1706529591615_0013\n24/01/30 03:13:32 INFO mapreduce.Job: Job job_1706529591615_0013 running in uber mode : false\n24/01/30 03:13:32 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:13:40 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:13:47 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:13:53 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 03:13:54 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:13:59 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:14:02 INFO mapreduce.Job: Job job_1706529591615_0013 completed successfully\n24/01/30 03:14:02 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161496\n\t\tTotal time spent by all reduces in occupied slots (ms)=8370\n\t\tTotal time spent by all map tasks (ms)=53832\n\t\tTotal time spent by all reduce tasks (ms)=2790\n\t\tTotal vcore-milliseconds taken by all map tasks=53832\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2790\n\t\tTotal megabyte-milliseconds taken by all map tasks=165371904\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8570880\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1484\n\t\tCPU time spent (ms)=18300\n\t\tPhysical memory (bytes) snapshot=5498171392\n\t\tVirtual memory (bytes) snapshot=44197384192\n\t\tTotal committed heap usage (bytes)=5076680704\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:14:02 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob2635846118909147971.jar tmpDir=null\n24/01/30 03:14:06 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:14:06 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:14:07 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:14:07 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:14:07 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:14:07 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:14:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0014\n24/01/30 03:14:08 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:14:08 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:14:08 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:14:08 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:14:08 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0014\n24/01/30 03:14:08 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0014/\n24/01/30 03:14:08 INFO mapreduce.Job: Running job: job_1706529591615_0014\n24/01/30 03:14:14 INFO mapreduce.Job: Job job_1706529591615_0014 running in uber mode : false\n24/01/30 03:14:14 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:14:22 INFO mapreduce.Job:  map 22% reduce 0%\n24/01/30 03:14:23 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:14:29 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:14:36 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:14:42 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:14:44 INFO mapreduce.Job: Job job_1706529591615_0014 completed successfully\n24/01/30 03:14:44 INFO mapreduce.Job: Counters: 49\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535983\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=160080\n\t\tTotal time spent by all reduces in occupied slots (ms)=8202\n\t\tTotal time spent by all map tasks (ms)=53360\n\t\tTotal time spent by all reduce tasks (ms)=2734\n\t\tTotal vcore-milliseconds taken by all map tasks=53360\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2734\n\t\tTotal megabyte-milliseconds taken by all map tasks=163921920\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8398848\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1463\n\t\tCPU time spent (ms)=17650\n\t\tPhysical memory (bytes) snapshot=5533024256\n\t\tVirtual memory (bytes) snapshot=44230848512\n\t\tTotal committed heap usage (bytes)=5155323904\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:14:44 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob433027537578533512.jar tmpDir=null\n24/01/30 03:14:48 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:14:48 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:14:49 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:14:49 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:14:49 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:14:49 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:14:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0015\n24/01/30 03:14:49 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:14:49 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:14:49 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:14:49 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:14:50 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0015\n24/01/30 03:14:50 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0015/\n24/01/30 03:14:50 INFO mapreduce.Job: Running job: job_1706529591615_0015\n24/01/30 03:14:57 INFO mapreduce.Job: Job job_1706529591615_0015 running in uber mode : false\n24/01/30 03:14:57 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:15:05 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:15:12 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:15:18 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 03:15:19 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:15:24 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:15:27 INFO mapreduce.Job: Job job_1706529591615_0015 completed successfully\n24/01/30 03:15:27 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643930\n\t\tFILE: Number of bytes written=5535973\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=284\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161643\n\t\tTotal time spent by all reduces in occupied slots (ms)=8310\n\t\tTotal time spent by all map tasks (ms)=53881\n\t\tTotal time spent by all reduce tasks (ms)=2770\n\t\tTotal vcore-milliseconds taken by all map tasks=53881\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2770\n\t\tTotal megabyte-milliseconds taken by all map tasks=165522432\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8509440\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1643978\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1643978\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1484\n\t\tCPU time spent (ms)=17620\n\t\tPhysical memory (bytes) snapshot=5460705280\n\t\tVirtual memory (bytes) snapshot=44230004736\n\t\tTotal committed heap usage (bytes)=5076156416\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=284\n24/01/30 03:15:27 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob8278338832535039640.jar tmpDir=null\n24/01/30 03:15:31 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:15:31 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:15:31 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:15:31 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:15:32 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:15:32 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:15:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0016\n24/01/30 03:15:32 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:15:32 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:15:32 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:15:32 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:15:32 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0016\n24/01/30 03:15:32 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0016/\n24/01/30 03:15:32 INFO mapreduce.Job: Running job: job_1706529591615_0016\n24/01/30 03:15:38 INFO mapreduce.Job: Job job_1706529591615_0016 running in uber mode : false\n24/01/30 03:15:38 INFO mapreduce.Job:  map 0% reduce 0%\n^C\nDeleted /user/root/hw1/test-output\n^C\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7260846506380348777.jar tmpDir=null\n24/01/30 03:15:44 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:15:45 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n^C\nThe slowest run took 6.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n33 s \u00b1 15.2 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"}], "source": "%%timeit\n!hdfs dfs -rm -r {HDFS_DIR}/test-output\n\n!hadoop jar {JAR_FILE} \\\n  -D stream.num.map.output.key.fields=1 \\\n  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n  -files /mapper_temp.py,/reducer_temp.py \\\n  -mapper mapper_temp.py \\\n  -reducer reducer_temp.py \\\n  -input {HDFS_DIR}/temperature_data.csv \\\n  -output {HDFS_DIR}/test-output \\\n  -numReduceTasks 1 \\\n  -cmdenv PATH={PATH}"}, {"cell_type": "code", "execution_count": 128, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "rm: `/user/root/hw1/test-output': No such file or directory\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob6367409978192137150.jar tmpDir=null\n24/01/30 03:15:51 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:15:52 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:15:52 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:15:52 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:15:53 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:15:54 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:15:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0017\n24/01/30 03:15:55 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:15:55 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:15:55 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:15:55 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:15:55 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0017\n24/01/30 03:15:55 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0017/\n24/01/30 03:15:55 INFO mapreduce.Job: Running job: job_1706529591615_0017\n24/01/30 03:16:26 INFO mapreduce.Job: Job job_1706529591615_0017 running in uber mode : false\n24/01/30 03:16:26 INFO mapreduce.Job:  map 0% reduce 0%\n^C\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob4846471439375809805.jar tmpDir=null\n24/01/30 03:16:38 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:38 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:39 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:39 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:40 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:16:41 INFO mapreduce.JobSubmitter: number of splits:9\nrm: `/user/root/hw1/test-output': No such file or directory\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob1478098130906388172.jar tmpDir=null\n24/01/30 03:16:48 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:48 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:49 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:49 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:50 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:16:50 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:16:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0019\n24/01/30 03:16:51 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:16:51 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:16:51 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:16:51 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n^C\nrm: `/user/root/hw1/test-output': No such file or directory\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob6138109765219064315.jar tmpDir=null\n24/01/30 03:16:56 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:56 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:57 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:16:57 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:16:58 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:16:59 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:17:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0020\n24/01/30 03:17:00 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:17:00 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:17:00 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:17:00 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:17:00 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0020\n24/01/30 03:17:00 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0020/\n24/01/30 03:17:00 INFO mapreduce.Job: Running job: job_1706529591615_0020\n24/01/30 03:18:03 INFO mapreduce.Job: Job job_1706529591615_0020 running in uber mode : false\n24/01/30 03:18:03 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:18:10 INFO mapreduce.Job:  map 11% reduce 0%\n24/01/30 03:18:11 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:18:17 INFO mapreduce.Job:  map 44% reduce 0%\n24/01/30 03:18:18 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:18:25 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:18:32 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:18:34 INFO mapreduce.Job: Job job_1706529591615_0020 completed successfully\n24/01/30 03:18:34 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643936\n\t\tFILE: Number of bytes written=5761017\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=282\n\t\tHDFS: Number of read operations=37\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=8\n\tJob Counters \n\t\tKilled reduce tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=2\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=163632\n\t\tTotal time spent by all reduces in occupied slots (ms)=23490\n\t\tTotal time spent by all map tasks (ms)=54544\n\t\tTotal time spent by all reduce tasks (ms)=7830\n\t\tTotal vcore-milliseconds taken by all map tasks=54544\n\t\tTotal vcore-milliseconds taken by all reduce tasks=7830\n\t\tTotal megabyte-milliseconds taken by all map tasks=167559168\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=24053760\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644032\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644032\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =18\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=18\n\t\tGC time elapsed (ms)=1673\n\t\tCPU time spent (ms)=20460\n\t\tPhysical memory (bytes) snapshot=5842079744\n\t\tVirtual memory (bytes) snapshot=48752775168\n\t\tTotal committed heap usage (bytes)=5361893376\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=282\n24/01/30 03:18:34 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob958893274654055406.jar tmpDir=null\n24/01/30 03:18:38 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:18:38 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:18:38 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:18:38 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:18:39 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:18:39 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:18:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0021\n24/01/30 03:18:39 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:18:39 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:18:39 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:18:39 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:18:39 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0021\n24/01/30 03:18:39 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0021/\n24/01/30 03:18:39 INFO mapreduce.Job: Running job: job_1706529591615_0021\n24/01/30 03:18:46 INFO mapreduce.Job: Job job_1706529591615_0021 running in uber mode : false\n24/01/30 03:18:46 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:18:54 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:19:01 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:19:07 INFO mapreduce.Job:  map 89% reduce 0%\n24/01/30 03:19:08 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:19:15 INFO mapreduce.Job:  map 100% reduce 50%\n24/01/30 03:19:16 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:19:18 INFO mapreduce.Job: Job job_1706529591615_0021 completed successfully\n24/01/30 03:19:18 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643936\n\t\tFILE: Number of bytes written=5761006\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=282\n\t\tHDFS: Number of read operations=37\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=6\n\tJob Counters \n\t\tKilled reduce tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=2\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=152136\n\t\tTotal time spent by all reduces in occupied slots (ms)=22917\n\t\tTotal time spent by all map tasks (ms)=50712\n\t\tTotal time spent by all reduce tasks (ms)=7639\n\t\tTotal vcore-milliseconds taken by all map tasks=50712\n\t\tTotal vcore-milliseconds taken by all reduce tasks=7639\n\t\tTotal megabyte-milliseconds taken by all map tasks=155787264\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=23467008\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644032\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644032\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =18\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=18\n\t\tGC time elapsed (ms)=1602\n\t\tCPU time spent (ms)=19630\n\t\tPhysical memory (bytes) snapshot=5833416704\n\t\tVirtual memory (bytes) snapshot=48643022848\n\t\tTotal committed heap usage (bytes)=5440012288\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=282\n24/01/30 03:19:18 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob2044093406563828879.jar tmpDir=null\n24/01/30 03:19:22 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:19:22 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:19:22 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:19:22 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:19:22 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:19:22 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:19:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0022\n24/01/30 03:19:23 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:19:23 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:19:23 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:19:23 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:19:23 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0022\n24/01/30 03:19:23 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0022/\n24/01/30 03:19:23 INFO mapreduce.Job: Running job: job_1706529591615_0022\n24/01/30 03:19:30 INFO mapreduce.Job: Job job_1706529591615_0022 running in uber mode : false\n24/01/30 03:19:30 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:19:38 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:19:45 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:19:51 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 03:19:52 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:19:58 INFO mapreduce.Job:  map 100% reduce 50%\n24/01/30 03:20:00 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:20:02 INFO mapreduce.Job: Job job_1706529591615_0022 completed successfully\n24/01/30 03:20:02 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643936\n\t\tFILE: Number of bytes written=5761017\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=282\n\t\tHDFS: Number of read operations=37\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=6\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=2\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161139\n\t\tTotal time spent by all reduces in occupied slots (ms)=23061\n\t\tTotal time spent by all map tasks (ms)=53713\n\t\tTotal time spent by all reduce tasks (ms)=7687\n\t\tTotal vcore-milliseconds taken by all map tasks=53713\n\t\tTotal vcore-milliseconds taken by all reduce tasks=7687\n\t\tTotal megabyte-milliseconds taken by all map tasks=165006336\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=23614464\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644032\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644032\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =18\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=18\n\t\tGC time elapsed (ms)=1523\n\t\tCPU time spent (ms)=20480\n\t\tPhysical memory (bytes) snapshot=5813121024\n\t\tVirtual memory (bytes) snapshot=48751804416\n\t\tTotal committed heap usage (bytes)=5243404288\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=282\n24/01/30 03:20:02 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob3898229748950672579.jar tmpDir=null\n24/01/30 03:20:06 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:20:06 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:20:07 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:20:07 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:20:07 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:20:07 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:20:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0023\n24/01/30 03:20:08 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:20:08 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:20:08 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:20:08 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:20:08 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0023\n24/01/30 03:20:08 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0023/\n24/01/30 03:20:08 INFO mapreduce.Job: Running job: job_1706529591615_0023\n24/01/30 03:20:14 INFO mapreduce.Job: Job job_1706529591615_0023 running in uber mode : false\n24/01/30 03:20:14 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:20:22 INFO mapreduce.Job:  map 11% reduce 0%\n24/01/30 03:20:23 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:20:29 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:20:36 INFO mapreduce.Job:  map 89% reduce 0%\n24/01/30 03:20:37 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:20:43 INFO mapreduce.Job:  map 100% reduce 50%\n24/01/30 03:20:44 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:20:46 INFO mapreduce.Job: Job job_1706529591615_0023 completed successfully\n24/01/30 03:20:46 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643936\n\t\tFILE: Number of bytes written=5761017\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=282\n\t\tHDFS: Number of read operations=37\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=6\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=2\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=164058\n\t\tTotal time spent by all reduces in occupied slots (ms)=21081\n\t\tTotal time spent by all map tasks (ms)=54686\n\t\tTotal time spent by all reduce tasks (ms)=7027\n\t\tTotal vcore-milliseconds taken by all map tasks=54686\n\t\tTotal vcore-milliseconds taken by all reduce tasks=7027\n\t\tTotal megabyte-milliseconds taken by all map tasks=167995392\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=21586944\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644032\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644032\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =18\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=18\n\t\tGC time elapsed (ms)=1618\n\t\tCPU time spent (ms)=20120\n\t\tPhysical memory (bytes) snapshot=5809606656\n\t\tVirtual memory (bytes) snapshot=48644861952\n\t\tTotal committed heap usage (bytes)=5358747648\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=282\n24/01/30 03:20:46 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7149526245830988345.jar tmpDir=null\n24/01/30 03:20:50 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:20:50 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:20:51 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:20:51 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:20:51 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:20:52 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:20:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0024\n24/01/30 03:20:52 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:20:52 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:20:52 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:20:52 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:20:52 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0024\n24/01/30 03:20:52 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0024/\n24/01/30 03:20:52 INFO mapreduce.Job: Running job: job_1706529591615_0024\n24/01/30 03:20:59 INFO mapreduce.Job: Job job_1706529591615_0024 running in uber mode : false\n24/01/30 03:20:59 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:21:07 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:21:14 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:21:21 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:21:27 INFO mapreduce.Job:  map 100% reduce 50%\n24/01/30 03:21:29 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:21:32 INFO mapreduce.Job: Job job_1706529591615_0024 completed successfully\n24/01/30 03:21:32 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643936\n\t\tFILE: Number of bytes written=5761017\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=282\n\t\tHDFS: Number of read operations=37\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=6\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=2\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=160686\n\t\tTotal time spent by all reduces in occupied slots (ms)=24777\n\t\tTotal time spent by all map tasks (ms)=53562\n\t\tTotal time spent by all reduce tasks (ms)=8259\n\t\tTotal vcore-milliseconds taken by all map tasks=53562\n\t\tTotal vcore-milliseconds taken by all reduce tasks=8259\n\t\tTotal megabyte-milliseconds taken by all map tasks=164542464\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=25371648\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644032\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644032\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =18\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=18\n\t\tGC time elapsed (ms)=1529\n\t\tCPU time spent (ms)=19490\n\t\tPhysical memory (bytes) snapshot=5834526720\n\t\tVirtual memory (bytes) snapshot=48640569344\n\t\tTotal committed heap usage (bytes)=5463080960\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=282\n24/01/30 03:21:32 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nThe slowest run took 10.97 times longer than the fastest. This could mean that an intermediate result is being cached.\n43 s \u00b1 28.9 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"}], "source": "%%timeit\n!hdfs dfs -rm -r {HDFS_DIR}/test-output\n\n!hadoop jar {JAR_FILE} \\\n  -D stream.num.map.output.key.fields=1 \\\n  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n  -files /mapper_temp.py,/reducer_temp.py \\\n  -mapper mapper_temp.py \\\n  -reducer reducer_temp.py \\\n  -input {HDFS_DIR}/temperature_data.csv \\\n  -output {HDFS_DIR}/test-output \\\n  -numReduceTasks 2 \\\n  -cmdenv PATH={PATH}"}, {"cell_type": "code", "execution_count": 129, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7766429566709621026.jar tmpDir=null\n24/01/30 03:21:36 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:21:36 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:21:37 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:21:37 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:21:37 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:21:37 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:21:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0025\n24/01/30 03:21:38 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:21:38 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:21:38 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:21:38 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:21:38 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0025\n24/01/30 03:21:38 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0025/\n24/01/30 03:21:38 INFO mapreduce.Job: Running job: job_1706529591615_0025\n24/01/30 03:21:45 INFO mapreduce.Job: Job job_1706529591615_0025 running in uber mode : false\n24/01/30 03:21:45 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:21:53 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:22:00 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:22:07 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:22:14 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:22:16 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:22:17 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:22:19 INFO mapreduce.Job: Job job_1706529591615_0025 completed successfully\n24/01/30 03:22:19 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=163257\n\t\tTotal time spent by all reduces in occupied slots (ms)=40935\n\t\tTotal time spent by all map tasks (ms)=54419\n\t\tTotal time spent by all reduce tasks (ms)=13645\n\t\tTotal vcore-milliseconds taken by all map tasks=54419\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13645\n\t\tTotal megabyte-milliseconds taken by all map tasks=167175168\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=41917440\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1678\n\t\tCPU time spent (ms)=22140\n\t\tPhysical memory (bytes) snapshot=6119440384\n\t\tVirtual memory (bytes) snapshot=53067018240\n\t\tTotal committed heap usage (bytes)=5678039040\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:22:19 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob2686241728636121766.jar tmpDir=null\n24/01/30 03:22:23 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:22:23 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:22:24 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:22:24 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:22:24 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:22:25 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:22:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0026\n24/01/30 03:22:25 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:22:25 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:22:25 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:22:25 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:22:25 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0026\n24/01/30 03:22:25 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0026/\n24/01/30 03:22:25 INFO mapreduce.Job: Running job: job_1706529591615_0026\n24/01/30 03:22:31 INFO mapreduce.Job: Job job_1706529591615_0026 running in uber mode : false\n24/01/30 03:22:31 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:22:39 INFO mapreduce.Job:  map 22% reduce 0%\n24/01/30 03:22:40 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:22:46 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:22:53 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:23:01 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:23:02 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:23:03 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:23:05 INFO mapreduce.Job: Job job_1706529591615_0026 completed successfully\n24/01/30 03:23:06 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161397\n\t\tTotal time spent by all reduces in occupied slots (ms)=45711\n\t\tTotal time spent by all map tasks (ms)=53799\n\t\tTotal time spent by all reduce tasks (ms)=15237\n\t\tTotal vcore-milliseconds taken by all map tasks=53799\n\t\tTotal vcore-milliseconds taken by all reduce tasks=15237\n\t\tTotal megabyte-milliseconds taken by all map tasks=165270528\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=46808064\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1802\n\t\tCPU time spent (ms)=22650\n\t\tPhysical memory (bytes) snapshot=6143578112\n\t\tVirtual memory (bytes) snapshot=53113348096\n\t\tTotal committed heap usage (bytes)=5787090944\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:23:06 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7804098096411514234.jar tmpDir=null\n24/01/30 03:23:09 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:23:10 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:23:10 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:23:10 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:23:10 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:23:11 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:23:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0027\n24/01/30 03:23:12 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:23:12 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:23:12 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:23:12 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:23:12 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0027\n24/01/30 03:23:12 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0027/\n24/01/30 03:23:12 INFO mapreduce.Job: Running job: job_1706529591615_0027\n24/01/30 03:23:19 INFO mapreduce.Job: Job job_1706529591615_0027 running in uber mode : false\n24/01/30 03:23:19 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:23:27 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:23:34 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:23:40 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 03:23:41 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:23:47 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:23:48 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:23:49 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:23:51 INFO mapreduce.Job: Job job_1706529591615_0027 completed successfully\n24/01/30 03:23:51 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=162795\n\t\tTotal time spent by all reduces in occupied slots (ms)=40221\n\t\tTotal time spent by all map tasks (ms)=54265\n\t\tTotal time spent by all reduce tasks (ms)=13407\n\t\tTotal vcore-milliseconds taken by all map tasks=54265\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13407\n\t\tTotal megabyte-milliseconds taken by all map tasks=166702080\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=41186304\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1686\n\t\tCPU time spent (ms)=21970\n\t\tPhysical memory (bytes) snapshot=6207676416\n\t\tVirtual memory (bytes) snapshot=53128511488\n\t\tTotal committed heap usage (bytes)=5751963648\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:23:51 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob5599014901235491712.jar tmpDir=null\n24/01/30 03:23:55 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:23:55 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:23:55 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:23:55 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:23:56 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:23:56 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:23:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0028\n24/01/30 03:23:56 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:23:56 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:23:56 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:23:56 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:23:56 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0028\n24/01/30 03:23:57 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0028/\n24/01/30 03:23:57 INFO mapreduce.Job: Running job: job_1706529591615_0028\n24/01/30 03:24:04 INFO mapreduce.Job: Job job_1706529591615_0028 running in uber mode : false\n24/01/30 03:24:04 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:24:12 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:24:19 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:24:25 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 03:24:26 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:24:32 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:24:34 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:24:35 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:24:37 INFO mapreduce.Job: Job job_1706529591615_0028 completed successfully\n24/01/30 03:24:37 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled reduce tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161178\n\t\tTotal time spent by all reduces in occupied slots (ms)=38109\n\t\tTotal time spent by all map tasks (ms)=53726\n\t\tTotal time spent by all reduce tasks (ms)=12703\n\t\tTotal vcore-milliseconds taken by all map tasks=53726\n\t\tTotal vcore-milliseconds taken by all reduce tasks=12703\n\t\tTotal megabyte-milliseconds taken by all map tasks=165046272\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=39023616\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1699\n\t\tCPU time spent (ms)=21850\n\t\tPhysical memory (bytes) snapshot=6194765824\n\t\tVirtual memory (bytes) snapshot=53097340928\n\t\tTotal committed heap usage (bytes)=5883035648\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:24:37 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob6249082441743346250.jar tmpDir=null\n24/01/30 03:24:41 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:24:41 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:24:41 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:24:41 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:24:42 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:24:42 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:24:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0029\n24/01/30 03:24:42 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:24:42 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:24:42 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:24:42 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:24:42 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0029\n24/01/30 03:24:42 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0029/\n24/01/30 03:24:42 INFO mapreduce.Job: Running job: job_1706529591615_0029\n24/01/30 03:24:49 INFO mapreduce.Job: Job job_1706529591615_0029 running in uber mode : false\n24/01/30 03:24:49 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:24:57 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:25:05 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:25:12 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:25:18 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:25:20 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:25:21 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:25:23 INFO mapreduce.Job: Job job_1706529591615_0029 completed successfully\n24/01/30 03:25:23 INFO mapreduce.Job: Counters: 49\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=158823\n\t\tTotal time spent by all reduces in occupied slots (ms)=39669\n\t\tTotal time spent by all map tasks (ms)=52941\n\t\tTotal time spent by all reduce tasks (ms)=13223\n\t\tTotal vcore-milliseconds taken by all map tasks=52941\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13223\n\t\tTotal megabyte-milliseconds taken by all map tasks=162634752\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=40621056\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1654\n\t\tCPU time spent (ms)=20900\n\t\tPhysical memory (bytes) snapshot=5903708160\n\t\tVirtual memory (bytes) snapshot=53068541952\n\t\tTotal committed heap usage (bytes)=5246025728\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:25:23 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob4285067888637728508.jar tmpDir=null\n24/01/30 03:25:27 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:25:27 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:25:27 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:25:27 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:25:27 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:25:27 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:25:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0030\n24/01/30 03:25:28 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:25:28 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:25:28 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:25:28 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:25:28 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0030\n24/01/30 03:25:28 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0030/\n24/01/30 03:25:28 INFO mapreduce.Job: Running job: job_1706529591615_0030\n24/01/30 03:25:34 INFO mapreduce.Job: Job job_1706529591615_0030 running in uber mode : false\n24/01/30 03:25:34 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:25:42 INFO mapreduce.Job:  map 22% reduce 0%\n24/01/30 03:25:43 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:25:49 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:25:57 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:26:04 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:26:06 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:26:07 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:26:08 INFO mapreduce.Job: Job job_1706529591615_0030 completed successfully\n24/01/30 03:26:08 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161091\n\t\tTotal time spent by all reduces in occupied slots (ms)=39366\n\t\tTotal time spent by all map tasks (ms)=53697\n\t\tTotal time spent by all reduce tasks (ms)=13122\n\t\tTotal vcore-milliseconds taken by all map tasks=53697\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13122\n\t\tTotal megabyte-milliseconds taken by all map tasks=164957184\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=40310784\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1568\n\t\tCPU time spent (ms)=21520\n\t\tPhysical memory (bytes) snapshot=6159798272\n\t\tVirtual memory (bytes) snapshot=53095112704\n\t\tTotal committed heap usage (bytes)=5798100992\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:26:08 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob8738414382376253968.jar tmpDir=null\n24/01/30 03:26:12 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:26:12 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:26:12 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:26:12 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:26:12 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:26:12 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:26:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0031\n24/01/30 03:26:13 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:26:13 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:26:13 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:26:13 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:26:13 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0031\n24/01/30 03:26:13 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0031/\n24/01/30 03:26:13 INFO mapreduce.Job: Running job: job_1706529591615_0031\n24/01/30 03:26:20 INFO mapreduce.Job: Job job_1706529591615_0031 running in uber mode : false\n24/01/30 03:26:20 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:26:28 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:26:35 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:26:42 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:26:49 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:26:51 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:26:52 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:26:53 INFO mapreduce.Job: Job job_1706529591615_0031 completed successfully\n24/01/30 03:26:53 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled reduce tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=161448\n\t\tTotal time spent by all reduces in occupied slots (ms)=40104\n\t\tTotal time spent by all map tasks (ms)=53816\n\t\tTotal time spent by all reduce tasks (ms)=13368\n\t\tTotal vcore-milliseconds taken by all map tasks=53816\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13368\n\t\tTotal megabyte-milliseconds taken by all map tasks=165322752\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=41066496\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1501\n\t\tCPU time spent (ms)=20880\n\t\tPhysical memory (bytes) snapshot=6067400704\n\t\tVirtual memory (bytes) snapshot=53111631872\n\t\tTotal committed heap usage (bytes)=5666504704\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:26:53 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\nDeleted /user/root/hw1/test-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob3743764931038666754.jar tmpDir=null\n24/01/30 03:26:57 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:26:57 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:26:58 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:26:58 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:26:58 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:26:58 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:26:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0032\n24/01/30 03:26:59 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:26:59 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:26:59 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:26:59 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:26:59 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0032\n24/01/30 03:26:59 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0032/\n24/01/30 03:26:59 INFO mapreduce.Job: Running job: job_1706529591615_0032\n24/01/30 03:27:06 INFO mapreduce.Job: Job job_1706529591615_0032 running in uber mode : false\n24/01/30 03:27:06 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 03:27:14 INFO mapreduce.Job:  map 22% reduce 0%\n24/01/30 03:27:15 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 03:27:21 INFO mapreduce.Job:  map 44% reduce 0%\n24/01/30 03:27:22 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 03:27:28 INFO mapreduce.Job:  map 89% reduce 0%\n24/01/30 03:27:29 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 03:27:35 INFO mapreduce.Job:  map 100% reduce 33%\n24/01/30 03:27:37 INFO mapreduce.Job:  map 100% reduce 67%\n24/01/30 03:27:38 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 03:27:40 INFO mapreduce.Job: Job job_1706529591615_0032 completed successfully\n24/01/30 03:27:40 INFO mapreduce.Job: Counters: 50\n\tFile System Counters\n\t\tFILE: Number of bytes read=1643942\n\t\tFILE: Number of bytes written=5986051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=1477664\n\t\tHDFS: Number of bytes written=283\n\t\tHDFS: Number of read operations=42\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=9\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=3\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=163479\n\t\tTotal time spent by all reduces in occupied slots (ms)=40491\n\t\tTotal time spent by all map tasks (ms)=54493\n\t\tTotal time spent by all reduce tasks (ms)=13497\n\t\tTotal vcore-milliseconds taken by all map tasks=54493\n\t\tTotal vcore-milliseconds taken by all reduce tasks=13497\n\t\tTotal megabyte-milliseconds taken by all map tasks=167402496\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=41462784\n\tMap-Reduce Framework\n\t\tMap input records=100000\n\t\tMap output records=100000\n\t\tMap output bytes=1443924\n\t\tMap output materialized bytes=1644086\n\t\tInput split bytes=972\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=1644086\n\t\tReduce input records=100000\n\t\tReduce output records=10\n\t\tSpilled Records=200000\n\t\tShuffled Maps =27\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=27\n\t\tGC time elapsed (ms)=1647\n\t\tCPU time spent (ms)=22570\n\t\tPhysical memory (bytes) snapshot=6106058752\n\t\tVirtual memory (bytes) snapshot=53233836032\n\t\tTotal committed heap usage (bytes)=5584191488\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=1476692\n\tFile Output Format Counters \n\t\tBytes Written=283\n24/01/30 03:27:40 INFO streaming.StreamJob: Output directory: /user/root/hw1/test-output\n45.8 s \u00b1 500 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"}], "source": "%%timeit\n!hdfs dfs -rm -r {HDFS_DIR}/test-output\n\n!hadoop jar {JAR_FILE} \\\n  -D stream.num.map.output.key.fields=1 \\\n  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n  -files /mapper_temp.py,/reducer_temp.py \\\n  -mapper mapper_temp.py \\\n  -reducer reducer_temp.py \\\n  -input {HDFS_DIR}/temperature_data.csv \\\n  -output {HDFS_DIR}/test-output \\\n  -numReduceTasks 3 \\\n  -cmdenv PATH={PATH}"}, {"cell_type": "markdown", "metadata": {}, "source": "# Question 6: Hadoop Streaming for Analyzing Employee Salary by Department\n\nIn this exercise, you'll work with the `employees_data.csv` dataset that contains employee information. Each record in the dataset has three fields - Employee Name, Department, and Salary. The goal is to analyze this data to find the average salary per department, sorted first by department name and then by average salary in descending order.\n\n### Q6 Tasks:\n* __a) mapper and reducer scripts development:__ Write two Python scripts, `mapper_employee.py` and `reducer_employee.py`. The mapper script should process each line of the input dataset and output the department and salary. The reducer script should calculate the average salary for each department.\n\n* __b) unit testing:__ Make sure you unit test the running of your mapper and reducer scripts using a small dataset like we did before with `alice_test.txt` or `hw1_test.txt`. This time, you will want to create a small data file. _Challenge: see if you can use Unix commands to output the first 5 lines of the provided dataset to a new test file!_\n\n* __c) run a Hadoop Streaming job:__ Write the correct command to execute the MapReduce job, ensuring the output is sorted first by department and then by average salary in descending order. After running the job, inspect the output to verify correctness.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part a - Do your work in mapper_employee.py and reducer_employee.py\n"}, {"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [], "source": "# part b - TODO: show your unit test here"}, {"cell_type": "code", "execution_count": 133, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing /hw1/employee_test.csv\n"}], "source": "%%writefile /hw1/employee_test.csv \nEve,Marketing,119144\nJulia,Customer Service,37393\nDavid,Human Resources,59780\nHannah,Engineering,149301\nFrank,Customer Service,99634\nBob,IT,140526\nIvan,Human Resources,85833\nBob,Customer Service,74054\nCharlie,Customer Service,116957\nGrace,Engineering,126260"}, {"cell_type": "code", "execution_count": 134, "metadata": {}, "outputs": [], "source": "!chmod a+x /mapper_employee.py /reducer_employee.py"}, {"cell_type": "code", "execution_count": 135, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Marketing\t119144\nCustomer Service\t37393\nHuman Resources\t59780\nEngineering\t149301\nCustomer Service\t99634\nIT\t140526\nHuman Resources\t85833\nCustomer Service\t74054\nCustomer Service\t116957\nEngineering\t126260\n"}], "source": "!cat /hw1/employee_test.csv | /mapper_employee.py"}, {"cell_type": "code", "execution_count": 142, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Marketing\t119144.0\nCustomer Service\t82009.5\nHuman Resources\t72806.5\nEngineering\t137780.5\nIT\t140526.0\n"}], "source": "!echo 'Marketing\t119144\\nCustomer Service\t37393\\nCustomer Service\t74054\\nCustomer Service\t116957\\nCustomer Service\t99634\\nHuman Resources\t59780\\nHuman Resources\t85833\\nEngineering\t149301\\nEngineering\t126260\\nIT\t140526' | /reducer_employee.py"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# part c - TODO: write the Hadoop Streaming command to test out your mapper and reducer scripts on the employee dataset\n"}, {"cell_type": "code", "execution_count": 143, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "rm: `/user/root/hw1/employee_data': No such file or directory\nput: `/user/root/hw1/employee_data.csv': File exists\n"}, {"data": {"text/plain": "'/opt/conda/anaconda/bin:/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'"}, "execution_count": 143, "metadata": {}, "output_type": "execute_result"}], "source": "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\nHDFS_DIR = \"/user/root/hw1\"\n!hdfs dfs -mkdir -p {HDFS_DIR}\n!hdfs dfs -rm -r {HDFS_DIR}/employee_data\n!hdfs dfs -put /hw1/employee_data.csv {HDFS_DIR}\n# store notebook environment path\nfrom os import environ\nPATH = environ['PATH']\nPATH"}, {"cell_type": "code", "execution_count": 145, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted /user/root/hw1/employee-output\npackageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7501998307273624396.jar tmpDir=null\n24/01/30 03:59:52 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:59:52 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:59:53 INFO client.RMProxy: Connecting to ResourceManager at cluster-0283-m/10.128.0.2:8032\n24/01/30 03:59:53 INFO client.AHSProxy: Connecting to Application History server at cluster-0283-m/10.128.0.2:10200\n24/01/30 03:59:53 INFO mapred.FileInputFormat: Total input files to process : 1\n24/01/30 03:59:53 INFO mapreduce.JobSubmitter: number of splits:9\n24/01/30 03:59:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706529591615_0036\n24/01/30 03:59:54 INFO conf.Configuration: resource-types.xml not found\n24/01/30 03:59:54 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n24/01/30 03:59:54 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n24/01/30 03:59:54 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n24/01/30 03:59:54 INFO impl.YarnClientImpl: Submitted application application_1706529591615_0036\n24/01/30 03:59:54 INFO mapreduce.Job: The url to track the job: http://cluster-0283-m:8088/proxy/application_1706529591615_0036/\n24/01/30 03:59:54 INFO mapreduce.Job: Running job: job_1706529591615_0036\n24/01/30 04:00:01 INFO mapreduce.Job: Job job_1706529591615_0036 running in uber mode : false\n24/01/30 04:00:01 INFO mapreduce.Job:  map 0% reduce 0%\n24/01/30 04:00:09 INFO mapreduce.Job:  map 22% reduce 0%\n24/01/30 04:00:10 INFO mapreduce.Job:  map 33% reduce 0%\n24/01/30 04:00:15 INFO mapreduce.Job:  map 44% reduce 0%\n24/01/30 04:00:17 INFO mapreduce.Job:  map 67% reduce 0%\n24/01/30 04:00:21 INFO mapreduce.Job:  map 78% reduce 0%\n24/01/30 04:00:23 INFO mapreduce.Job:  map 100% reduce 0%\n24/01/30 04:00:29 INFO mapreduce.Job:  map 100% reduce 100%\n24/01/30 04:00:31 INFO mapreduce.Job: Job job_1706529591615_0036 completed successfully\n24/01/30 04:00:31 INFO mapreduce.Job: Counters: 49\n\tFile System Counters\n\t\tFILE: Number of bytes read=3684374\n\t\tFILE: Number of bytes written=9617201\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=4478268\n\t\tHDFS: Number of bytes written=279\n\t\tHDFS: Number of read operations=32\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tLaunched map tasks=9\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=9\n\t\tTotal time spent by all maps in occupied slots (ms)=153600\n\t\tTotal time spent by all reduces in occupied slots (ms)=8724\n\t\tTotal time spent by all map tasks (ms)=51200\n\t\tTotal time spent by all reduce tasks (ms)=2908\n\t\tTotal vcore-milliseconds taken by all map tasks=51200\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2908\n\t\tTotal megabyte-milliseconds taken by all map tasks=157286400\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8933376\n\tMap-Reduce Framework\n\t\tMap input records=200000\n\t\tMap output records=200000\n\t\tMap output bytes=3284368\n\t\tMap output materialized bytes=3684422\n\t\tInput split bytes=945\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=10\n\t\tReduce shuffle bytes=3684422\n\t\tReduce input records=200000\n\t\tReduce output records=10\n\t\tSpilled Records=400000\n\t\tShuffled Maps =9\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9\n\t\tGC time elapsed (ms)=1351\n\t\tCPU time spent (ms)=21360\n\t\tPhysical memory (bytes) snapshot=5591515136\n\t\tVirtual memory (bytes) snapshot=44323229696\n\t\tTotal committed heap usage (bytes)=5169479680\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=4477323\n\tFile Output Format Counters \n\t\tBytes Written=279\n24/01/30 04:00:31 INFO streaming.StreamJob: Output directory: /user/root/hw1/employee-output\n"}], "source": "!hdfs dfs -rm -r {HDFS_DIR}/employee-output\n\n!hadoop jar {JAR_FILE} \\\n  -D stream.num.map.output.key.fields=1 \\\n  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n  -files /mapper_employee.py,/reducer_employee.py \\\n  -mapper mapper_employee.py \\\n  -reducer reducer_employee.py \\\n  -input {HDFS_DIR}/employee_data.csv \\\n  -output {HDFS_DIR}/employee-output \\\n  -numReduceTasks 1 \\\n  -cmdenv PATH={PATH}"}, {"cell_type": "code", "execution_count": 146, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\n-rw-r--r--   1 root hadoop          0 2024-01-30 04:00 /user/root/hw1/employee-output/_SUCCESS\n-rw-r--r--   1 root hadoop        279 2024-01-30 04:00 /user/root/hw1/employee-output/part-00000\nCustomer Service\t90295.63099630996\nEngineering\t89913.67894343384\nFinance\t90040.97204876598\nHuman Resources\t89908.50350982751\nIT\t89895.23108948636\nMarketing\t89665.25036021265\nOperations\t89939.54684923292\nProduct\t89907.30719683522\nResearch\t89811.8197848177\nSales\t90229.74844158455\n"}], "source": "!hdfs dfs -ls {HDFS_DIR}/employee-output\n!hdfs dfs -cat {HDFS_DIR}/employee-output/part-0000* > /results_employee.txt\n!head -n 20 /results_employee.txt"}, {"cell_type": "markdown", "metadata": {}, "source": "# Submission Instructions\nYou will need to submit a zip file to **Gradescope** containing the following files:\n- This notebook HW1.ipynb\n- From Question 2, final output of alice_counts.txt AND wordCount.py \n- From Question 5, mapper_temp.py and reducer_temp.py\n- From Question 6, mapper_employee.py and reducer_employee.py"}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "297px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": 4, "toc_cell": false, "toc_position": {"height": "951px", "left": "0px", "right": "1561px", "top": "106px", "width": "600px"}, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 4}