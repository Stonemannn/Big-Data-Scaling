{"cells":[{"cell_type":"markdown","metadata":{},"source":["## DS 5460 Big Data Scaling, SPRING 2024\n","\n","## Homework Assignment \\#2: Naive Bayes in MapReduce\n","\n","Objectives:\n","- Build proficiency with MapReduce concepts like mappers, reducers, and parallelization as they apply to a machine learning algorithm implementation\n","- Understand the Naive Bayes algorithm and how it can be parallelized effectively using the MapReduce framework\n","\n","Instructions:\n","\n","In this assignment you will implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your algorithm, you'll first test it on a small dataset. For the main task in this assignment you'll be working with a small subset of the Enron Spam/Ham Corpus. This assignment will be the only one in which you use Hadoop Streaming to implement a distributed algorithm. The key reason we continue to teach Hadoop streaming is because of the way it forces the programmer to think carefully about what is happening under the hood when you parallelize a calculation. \n","\n","Be sure to read all text cell descriptions and comments closely to fill in your solution when expected. In addition to the specified tasks, there may be `TODO` comments, for you to complete to set up your environment properly.\n","\n","We will only grade code written in the designated spaces. If a question's instructions are unclear, please reach out for clarification on Piazza. We expect each student to write their own code independently. If GenAI tools are used, **in which specific questions, commands, code, etc,. and how they were used must be disclosed properly in a separate text cell at the end of this notebook. Remember that the short answer solutions must be your independent work and should not be derived from GenAI tools.** \n","\n","__TIPS:__ \n","1. Make use of your peers and TAs by asking questions on Piazza. Everyone has different experiences and background so don't be shy; all questions are welcome!\n","2. Only use ONE (1) reducer for this assignment.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Notebook Setup\n","Before starting, run the following cells to confirm your setup."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hadoop 2.10.2\n","Subversion Unknown -r Unknown\n","Compiled by bigtop on 2023-09-02T20:18Z\n","Compiled with protoc 2.5.0\n","From source with checksum 4bb37aedd62b388ba458183bdd93130\n","This command was run using /usr/lib/hadoop/hadoop-common-2.10.2.jar\n"]}],"source":["!hadoop version"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%reload_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# TODO: setting up global vars (paths) - ADJUST AS NEEDED\n","JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n","HDFS_DIR = \"/user/root/HW2\"\n","# HOME_DIR = \"/media/notebooks/Assignments/HW2\"\n","# %cd {HOME_DIR}"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n","from os import environ\n","PATH  = environ['PATH']"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# TODO: path to dataset - ADJUST AS NEEDED\n","ENRON = \"email.txt\""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1 items\n","drwxr-xr-x   - root hadoop          0 2024-02-07 02:08 HW2\n"]}],"source":["# make the HDFS directory if it doesn't already exist\n","!hdfs dfs -mkdir -p {HDFS_DIR}\n","!hdfs dfs -ls "]},{"cell_type":"markdown","metadata":{},"source":["# About the Data\n","The main task is to train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW2 data directory (__`HW2/data/email.txt`__). (The original data is available [here](https://www2.aueb.gr/users/ion/data/enron-spam/), which was created by researchers working on personalized Bayesian spam filters.)\n","\n","__Format:__   \n","All messages are saved in a tab-delimited format:  \n","\n",">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n","\n","Data dictionary:  \n",">   `ID: (string) unique message identifier`  \n","    `SPAM: (binary) with 1 indicating a spam message and 0 indicating a ham (legitimate) message`  \n","    `SUBJECT: (string) title of the message`  \n","    `CONTENT: (string) content of the message`   \n","    \n","Note that `SUBJECT` or `CONTENT` may be \"NA\", and all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n","0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n","0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n","0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n","0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"]}],"source":["# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n","!head -n 5 {ENRON} | cut -c-100"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100 email.txt\n"]}],"source":["# check how many messages/lines are in the file \n","# (Note: this number may be off by 1 if the last line doesn't end with a newline)\n","!wc -l {ENRON}"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# load the data into HDFS (RUN THIS CELL AS IS)\n","!hdfs dfs -copyFromLocal {ENRON} {HDFS_DIR}/enron.txt"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1 items\n","-rw-r--r--   1 root hadoop     204559 2024-02-07 02:08 /user/root/HW2/enron.txt\n"]}],"source":["!hdfs dfs -ls {HDFS_DIR}"]},{"cell_type":"markdown","metadata":{},"source":["# Question 1:  Enron Ham/Spam EDA\n","Before building the classifier, we first need to explore this data to find out which words occur more in spam emails than in legitimate (\"ham\") emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. \n","\n","__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n","\n","### Tasks:\n","* __a) Implement mapper and reducer scripts:__ Complete the missing components of the code in __`eda/mapper.py`__ and __`eda/reducer.py`__ to create a Hadoop MapReduce job that counts how many times each word in the corpus occurs in an email for each class (spam vs ham). Pay close attention to the data format specified in the comments of these scripts. \n","\n","* __b) Unit test your scripts and run an MR job:__ Create unit tests to confirm that your code works as expected, then write the Hadoop Streaming command to apply your analysis to the actual Enron data. Save the results to a local file.\n","\n","* __c) Code in Notebook and answer below:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`part b`' and then report the answer in the Student Answers area provided below.)\n","\n","* __d) Code in Notebook and answer below:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file. Then describe in words below how you would go about printing the top 10 words in each class given this sorted output. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reordering the records from `part a`, you should be able to just use `/bin/cat` for both_. However, if the sorted output is not what you expect, you can duplicate your mapper.py as a mapper-by-class.py file and modify the emitted output like we did in class with the `Topwords` exercise, where we reordered the output to print count then word)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Q1 Student Answers:\n","> __c)__ assistance appears in spams 8 times and in hams 2 times.\n","\n","> __d)__ I can use commands: \n","\n","! cat sort_results.txt | sort -k2,2 -k3,3nr | head -n 10\n","\n","! cat sort_results.txt | sort -k2,2r -k3,3nr | head -n 10"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n","!chmod a+x mapper.py\n","!chmod a+x reducer.py"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["title\t1\t1\n","body\t1\t1\n","title\t0\t1\n","body\t0\t1\n"]}],"source":["# part a - unit test eda/mapper.py\n","# ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n","# You may use the following test case or create your own \n","!echo \"d1\t1\ttitle\tbody\\nd2\t0\ttitle\tbody\" | /mapper.py"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["one\t1\t3\n","one\t0\t1\n","two\t0\t1\n"]}],"source":["# part a - unit test eda/reducer.py\n","# word \\t class \\t partialCount \n","# You may use the following test case or create your own \n","\n","!echo \"one\t1\t1\\none\t1\t1\\none\t1\t1\\none\t0\t1\\ntwo\t0\t1\"  | /reducer.py"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted /user/root/HW2/eda-output\n"]}],"source":["# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n","!hdfs dfs -rm -r {HDFS_DIR}/eda-output"]},{"cell_type":"code","execution_count":55,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob4796811118056949793.jar tmpDir=null\n","24/02/07 04:03:01 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/07 04:03:01 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/07 04:03:02 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/07 04:03:02 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/07 04:03:02 INFO mapred.FileInputFormat: Total input files to process : 1\n","24/02/07 04:03:03 INFO mapreduce.JobSubmitter: number of splits:9\n","24/02/07 04:03:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707263943154_0004\n","24/02/07 04:03:04 INFO conf.Configuration: resource-types.xml not found\n","24/02/07 04:03:04 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/02/07 04:03:04 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/02/07 04:03:04 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/02/07 04:03:04 INFO impl.YarnClientImpl: Submitted application application_1707263943154_0004\n","24/02/07 04:03:04 INFO mapreduce.Job: The url to track the job: http://hw2hw2cluster-m:8088/proxy/application_1707263943154_0004/\n","24/02/07 04:03:04 INFO mapreduce.Job: Running job: job_1707263943154_0004\n","24/02/07 04:03:12 INFO mapreduce.Job: Job job_1707263943154_0004 running in uber mode : false\n","24/02/07 04:03:12 INFO mapreduce.Job:  map 0% reduce 0%\n","24/02/07 04:03:22 INFO mapreduce.Job:  map 33% reduce 0%\n","24/02/07 04:03:30 INFO mapreduce.Job:  map 44% reduce 0%\n","24/02/07 04:03:31 INFO mapreduce.Job:  map 67% reduce 0%\n","24/02/07 04:03:40 INFO mapreduce.Job:  map 100% reduce 0%\n","24/02/07 04:03:50 INFO mapreduce.Job:  map 100% reduce 33%\n","24/02/07 04:03:52 INFO mapreduce.Job:  map 100% reduce 100%\n","24/02/07 04:03:55 INFO mapreduce.Job: Job job_1707263943154_0004 completed successfully\n","24/02/07 04:03:55 INFO mapreduce.Job: Counters: 50\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=369016\n","\t\tFILE: Number of bytes written=3429623\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=258689\n","\t\tHDFS: Number of bytes written=70551\n","\t\tHDFS: Number of read operations=42\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=9\n","\tJob Counters \n","\t\tKilled map tasks=1\n","\t\tLaunched map tasks=9\n","\t\tLaunched reduce tasks=3\n","\t\tData-local map tasks=9\n","\t\tTotal time spent by all maps in occupied slots (ms)=216165\n","\t\tTotal time spent by all reduces in occupied slots (ms)=59259\n","\t\tTotal time spent by all map tasks (ms)=72055\n","\t\tTotal time spent by all reduce tasks (ms)=19753\n","\t\tTotal vcore-milliseconds taken by all map tasks=72055\n","\t\tTotal vcore-milliseconds taken by all reduce tasks=19753\n","\t\tTotal megabyte-milliseconds taken by all map tasks=221352960\n","\t\tTotal megabyte-milliseconds taken by all reduce tasks=60681216\n","\tMap-Reduce Framework\n","\t\tMap input records=100\n","\t\tMap output records=31490\n","\t\tMap output bytes=306018\n","\t\tMap output materialized bytes=369160\n","\t\tInput split bytes=882\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5065\n","\t\tReduce shuffle bytes=369160\n","\t\tReduce input records=31490\n","\t\tReduce output records=6060\n","\t\tSpilled Records=62980\n","\t\tShuffled Maps =27\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=27\n","\t\tGC time elapsed (ms)=2449\n","\t\tCPU time spent (ms)=27690\n","\t\tPhysical memory (bytes) snapshot=6299795456\n","\t\tVirtual memory (bytes) snapshot=53201596416\n","\t\tTotal committed heap usage (bytes)=5905055744\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=257807\n","\tFile Output Format Counters \n","\t\tBytes Written=70551\n","24/02/07 04:03:55 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"]}],"source":["# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n","!hadoop jar {JAR_FILE} \\\n","  -files /reducer.py,/mapper.py \\\n","  -mapper mapper.py \\\n","  -reducer reducer.py \\\n","  -input {HDFS_DIR}/enron.txt \\\n","  -output {HDFS_DIR}/eda-output \\\n","  -numReduceTasks 1 \\\n","  -cmdenv PATH={PATH}"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n","!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > results.txt"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["assistance\t1\t8\n","assistance\t0\t2\n"]}],"source":["# part b - write your grep command here\n","!grep \"assistance\" results.txt"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted /user/root/HW2/eda-sort-output\n"]}],"source":["# part d - clear the output directory in HDFS (RUN THIS CELL AS IS)\n","!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"]},{"cell_type":"code","execution_count":75,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob735695789868908910.jar tmpDir=null\n","24/02/07 04:37:13 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/07 04:37:14 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/07 04:37:14 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/07 04:37:14 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/07 04:37:15 INFO mapred.FileInputFormat: Total input files to process : 3\n","24/02/07 04:37:15 INFO mapreduce.JobSubmitter: number of splits:10\n","24/02/07 04:37:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707263943154_0008\n","24/02/07 04:37:16 INFO conf.Configuration: resource-types.xml not found\n","24/02/07 04:37:16 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/02/07 04:37:16 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/02/07 04:37:16 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/02/07 04:37:16 INFO impl.YarnClientImpl: Submitted application application_1707263943154_0008\n","24/02/07 04:37:16 INFO mapreduce.Job: The url to track the job: http://hw2hw2cluster-m:8088/proxy/application_1707263943154_0008/\n","24/02/07 04:37:16 INFO mapreduce.Job: Running job: job_1707263943154_0008\n","24/02/07 04:37:24 INFO mapreduce.Job: Job job_1707263943154_0008 running in uber mode : false\n","24/02/07 04:37:24 INFO mapreduce.Job:  map 0% reduce 0%\n","24/02/07 04:37:34 INFO mapreduce.Job:  map 30% reduce 0%\n","24/02/07 04:37:42 INFO mapreduce.Job:  map 40% reduce 0%\n","24/02/07 04:37:43 INFO mapreduce.Job:  map 60% reduce 0%\n","24/02/07 04:37:49 INFO mapreduce.Job:  map 70% reduce 0%\n","24/02/07 04:37:51 INFO mapreduce.Job:  map 90% reduce 0%\n","24/02/07 04:37:54 INFO mapreduce.Job:  map 100% reduce 0%\n","24/02/07 04:38:00 INFO mapreduce.Job:  map 100% reduce 100%\n","24/02/07 04:38:02 INFO mapreduce.Job: Job job_1707263943154_0008 completed successfully\n","24/02/07 04:38:03 INFO mapreduce.Job: Counters: 50\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=88737\n","\t\tFILE: Number of bytes written=2626788\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=97022\n","\t\tHDFS: Number of bytes written=76611\n","\t\tHDFS: Number of read operations=35\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\tJob Counters \n","\t\tKilled map tasks=1\n","\t\tLaunched map tasks=10\n","\t\tLaunched reduce tasks=1\n","\t\tData-local map tasks=10\n","\t\tTotal time spent by all maps in occupied slots (ms)=209886\n","\t\tTotal time spent by all reduces in occupied slots (ms)=9672\n","\t\tTotal time spent by all map tasks (ms)=69962\n","\t\tTotal time spent by all reduce tasks (ms)=3224\n","\t\tTotal vcore-milliseconds taken by all map tasks=69962\n","\t\tTotal vcore-milliseconds taken by all reduce tasks=3224\n","\t\tTotal megabyte-milliseconds taken by all map tasks=214923264\n","\t\tTotal megabyte-milliseconds taken by all reduce tasks=9904128\n","\tMap-Reduce Framework\n","\t\tMap input records=6060\n","\t\tMap output records=6060\n","\t\tMap output bytes=76611\n","\t\tMap output materialized bytes=88791\n","\t\tInput split bytes=1100\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=6060\n","\t\tReduce shuffle bytes=88791\n","\t\tReduce input records=6060\n","\t\tReduce output records=6060\n","\t\tSpilled Records=12120\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=1876\n","\t\tCPU time spent (ms)=22010\n","\t\tPhysical memory (bytes) snapshot=6047113216\n","\t\tVirtual memory (bytes) snapshot=48668737536\n","\t\tTotal committed heap usage (bytes)=5577900032\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=95922\n","\tFile Output Format Counters \n","\t\tBytes Written=76611\n","24/02/07 04:38:03 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"]}],"source":["# part d - write your Hadoop streaming job here\n","!hadoop jar {JAR_FILE} \\\n","  -D stream.num.map.output.key.fields=3 \\\n","  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n","  -D mapreduce.partition.keycomparator.options=\"-k2,2n -k3,3nr\" \\\n","  -mapper /bin/cat \\\n","  -input {HDFS_DIR}/eda-output/part-0000* \\\n","  -output {HDFS_DIR}/eda-sort-output \\\n","  -numReduceTasks 1 \\\n","  -cmdenv PATH={PATH}"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["!hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000* > sort_results.txt"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["the\t0\t549\t\n","to\t0\t398\t\n","ect\t0\t382\t\n","and\t0\t278\t\n","of\t0\t230\t\n","hou\t0\t206\t\n","a\t0\t196\t\n","in\t0\t182\t\n","for\t0\t170\t\n","on\t0\t135\t\n","sort: write failed: 'standard output': Broken pipe\n","sort: write error\n"]}],"source":["! cat sort_results.txt | sort -k2,2 -k3,3nr | head -n 10"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["the\t1\t698\t\n","to\t1\t566\t\n","and\t1\t392\t\n","your\t1\t357\t\n","a\t1\t347\t\n","you\t1\t345\t\n","of\t1\t336\t\n","in\t1\t236\t\n","for\t1\t204\t\n","com\t1\t153\t\n","sort: write failed: 'standard output': Broken pipe\n","sort: write error\n"]}],"source":["! cat sort_results.txt | sort -k2,2r -k3,3nr | head -n 10"]},{"cell_type":"markdown","metadata":{},"source":["# Question 2: Document Classification Task Overview\n","In this question we'll use the following example to 'train' a LaPlace (plus one) smoothed Multinomial Naive Bayes model and classify a test document. In addition to the lecture slides, you may also want to review [this chapter on text classification and Naive Bayes](https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf), which provides a thorough introduction to the document classification task and the math behind Naive Bayes. \n","\n","\n","### Tasks:\n","\n","* __a) spam email probability calculation:__ Below in the code cell labeled `part a)`, calculate the probability that an email is spam given that it contains only one word, \"Urgent\", from the vocabulary, using the provided probabilities. Additionally, spam is $40\\%$ of all email, and $80\\%$ of spam email contains the word “Urgent”, i.e, $Pr(Urgent|SPAM) =0.8.$\n","\n","\n","* __b) calculate probabilities for the classifier:__ Fill in the missing code in code cell labeled `part b)` to compute the probabilities used in the Multinomial Naive Bayes classifier with Laplace (plus one) smoothing. Use the following training dataset of 5 documents for a 2 Class problem: HAM versus SPAM.\n","\n","**Training Data**\n","\n","|DocId |Class | Document\n","|---|---|---|\n","|d1 | HAM | good\n","|d2 | SPAM | very good\n","|d3 | SPAM | good bad\n","|d4 | HAM | very bad\n","|d5 | SPAM | very bad very good\n","\n","\n","The vocabulary of the dataset is [good, very, bad]. \n","\n","* __c) Predict on the test data:__ Complete the code cell labeled `part c` to classify the test data )consisting of a single test case)\n","\n","__Test Data__\n","\n","| DocId | Class | Document String\n","|---|---|---|\n","| d6 | ?? | good bad very\n","\n","-----------------------------------\n","\n","* __d) Short answer below:__ Equation 13.3 in [this chapter on text classification and Naive Bayes](https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf) shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and short written explanation.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Q2 Student Answers:\n","\n","> __d)__ \n","$c$ = $argmax$$\\hat{P}(c|d)$ = $argmax$$\\hat{P}(c)\\space\\Pi{P}(t_i|c)$\n","\n","$\\hat{P}(c)$ which is Prior probability of hypothesis, and ${P}(t_i|c)$ which is the likehood are required.\n","\n","For each $\\hat{P}(c_i)$, select the class $c_k$ making the largest $\\hat{P}(c_i)$.\n","\n","\n"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Pr(SPAM|X=Urgent) = 0.6400000000000001\n"]}],"source":["# part a) TODO: Complete the calculation for Pr(SPAM|X=Urgent)\n","# Given probabilities\n","pr_urgent = 0.5  # Pr(\"Urgent\")\n","pr_spam = 0.4  # Pr(SPAM)\n","pr_urgent_given_spam = 0.8  # Pr(Urgent|SPAM)\n","\n","# TODO: your code here to calculate Pr(SPAM|X=Urgent)\n","pr_spam_given_urgent = pr_spam * pr_urgent_given_spam / pr_urgent\n","print(f'Pr(SPAM|X=Urgent) = {pr_spam_given_urgent}')"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>bad</th>\n","      <th>good</th>\n","      <th>very</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>d1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>d2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>d3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>d4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>d5</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Class  bad  good  very\n","d1      0    0     1     0\n","d2      1    0     1     1\n","d3      1    1     1     0\n","d4      0    1     0     1\n","d5      1    1     1     2"]},"metadata":{},"output_type":"display_data"}],"source":["# part b\n","\n","import pandas as pd\n","import numpy as np\n","\n","vocabulary = [\"bad\", \"good\", \"very\"]\n","\n","''' Here's the data again that we are representing below\n","|DocId |Class | Document\n","|---|---|---|\n","|d1 | HAM | good\n","|d2 | SPAM | very good\n","|d3 | SPAM | good bad\n","|d4 | HAM | very bad\n","|d5 | SPAM | very bad very good\n","'''\n","\n","# Document by word matrix\n","doc_by_word = np.array([[0, 1, 0],\n","                        [0, 1, 1],\n","                        [1, 1, 0],\n","                        [1, 0, 1],\n","                        [1, 1, 2]])\n","\n","# y_train: 0 for Ham and 1 for Spam\n","class_by_doc = np.array([0, 1, 1, 0, 1])\n","df = pd.DataFrame(np.c_[class_by_doc, doc_by_word], index = [\"d1\", \"d2\", \"d3\", \"d4\", \"d5\"], columns = [\"Class\"] + vocabulary)\n","display(pd.DataFrame(np.c_[class_by_doc, doc_by_word], index = [\"d1\", \"d2\", \"d3\", \"d4\", \"d5\"], columns = [\"Class\"] + vocabulary))"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model_priors: 1    0.6\n","0    0.4\n","Name: Class, dtype: float64\n"]}],"source":["# Learn the Naïve Bayes Classification:\n","model_priors = df['Class'].value_counts(normalize=True)\n","print(f\"model_priors: {model_priors}\") # Expected output - model_priors: [0.4 0.6]"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 1 0]\n"," [1 0 1]]\n","Pr(word_i|ham):  [0.333 0.333 0.333]\n"]}],"source":["# Calculate Pr(word_i|ham) or HAM class conditional probability, using LaPlace Smoothing\n","print(doc_by_word[class_by_doc == 0, :])\n","total_words_given_ham = np.sum(doc_by_word[class_by_doc == 0, :])\n","model_data_given_ham = (np.sum(doc_by_word[class_by_doc == 0, :],axis = 0)+1)/(total_words_given_ham+len(vocabulary))\n","print(f\"Pr(word_i|ham):  {np.round(model_data_given_ham, 3)}\")"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 1 1]\n"," [1 1 0]\n"," [1 1 2]]\n","Pr(word_i|spam):  [0.273 0.364 0.364]\n"]}],"source":["# Calculate Pr(word_i|spam) aka SPAM class conditionals:\n","print(doc_by_word[class_by_doc == 1, :])\n","total_words_given_spam = np.sum(doc_by_word[class_by_doc == 1, :])\n","model_data_given_spam = (np.sum(doc_by_word[class_by_doc == 1, :],axis = 0)+1)/(total_words_given_spam+len(vocabulary))\n","print(f\"Pr(word_i|spam):  {np.round(model_data_given_spam, 3)}\")"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bad</th>\n","      <th>good</th>\n","      <th>very</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>d6</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    bad  good  very\n","d6    1     1     1"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Posterior Probabilities in % is: Pr(Ham|D6) is :      41\n","Posterior Probabilities in % is: Pr(SPAM|D6) is :      59\n"]}],"source":["# part c\n","\n","# Test document terms are: bad, good, very\n","d6 = [1, 1, 1] # TEST DOCUMENT\n","display(pd.DataFrame([d6], index = [\"d6\"], columns = vocabulary))\n","\n","# Naïve Bayes Classification\n","# Likelihood\n","# Applying the Unigram (single word) Language Model\n","likelihood_d6_given_ham = np.prod(model_data_given_ham)\n","likelihood_d6_given_spam = np.prod(model_data_given_spam)\n","\n","# Calculate Posterior Probabilities using the learned Naive Bayes Model\n","pr_ham =  0.4 * likelihood_d6_given_ham\n","pr_spam = 0.6 * likelihood_d6_given_spam\n","\n","# Printing the normalized posterior probabilities\n","print(f\"Posterior Probabilities in % is: Pr(Ham|D6) is : {100 * pr_ham / (pr_spam + pr_ham):7.0f}\")\n","print(f\"Posterior Probabilities in % is: Pr(SPAM|D6) is : {100 * pr_spam / (pr_spam + pr_ham):7.0f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Question 3: Naive Bayes Inference\n","In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 8 below. In both of these questions we'll continue to use the example corpus from [this chapter on text classification and Naive Bayes](https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf) to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n","\n","### Tasks:\n","* __a) short answer and complete `part a` code cell:__ run the provided cells to create the example files and write the command to load them in to HDFS. Then take a closer look at __`NBmodel.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the keys and values in this file, and what do the fields represent?\n","\n","* __b) code mapper:__ Complete the code in __`model/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected.\n","\n","* __c) run a Hadoop job:__ Write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:`_You'll need to load the model file (NBmodel.txt) to do an in-memory join. This file can be added to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Q3 Student Answers:\n","\n","> __a)__ \n","- The keys in NBmodel are the city names. The first column represent the frequency of each word appearing in CLASS_0. The second column represents the frequency of each word appearing in CLASS_1. The third column represents the conditional probability of each word given CLASS_0 ($P(word|CLASS=0)$). The forth column represents the conditional probability of each word given CLASS_1 ($P(word|CLASS=1)$)\n","- For row:ClassPriors\t1.0,3.0,0.25,0.75. The values are frequency of CLASS_0, CLASS_1, probability of CLASS_0, probability of CLASS_1."]},{"cell_type":"markdown","metadata":{},"source":["Run these cells to create the example corpus and model."]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing model/chineseTrain.txt\n"]}],"source":["%%writefile model/chineseTrain.txt\n","D1\t1\t\tChinese Beijing Chinese\n","D2\t1\t\tChinese Chinese Shanghai\n","D3\t1\t\tChinese Macao\n","D4\t0\t\tTokyo Japan Chinese"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing model/chineseTest.txt\n"]}],"source":["%%writefile model/chineseTest.txt\n","D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n","D6\t1\t\tBeijing Shanghai Trade\n","D7\t0\t\tJapan Macao Tokyo\n","D8\t0\t\tTokyo Japan Trade"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing NBmodel.txt\n"]}],"source":["%%writefile NBmodel.txt\n","beijing\t0.0,1.0,0.111111111111,0.142857142857\n","chinese\t1.0,5.0,0.222222222222,0.428571428571\n","tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n","shanghai\t0.0,1.0,0.111111111111,0.142857142857\n","ClassPriors\t1.0,3.0,0.25,0.75\n","japan\t1.0,0.0,0.222222222222,0.0714285714286\n","macao\t0.0,1.0,0.111111111111,0.142857142857"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["# part a\n","# TODO: load the data files into HDFS\n","# write your commands below\n","!hdfs dfs -copyFromLocal model/chineseTrain.txt {HDFS_DIR}/chineseTrain.txt\n","!hdfs dfs -copyFromLocal model/chineseTest.txt {HDFS_DIR}/chineseTest.txt\n","!hdfs dfs -copyFromLocal NBmodel.txt {HDFS_DIR}/NBmodel.txt"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6 items\n","-rw-r--r--   1 root hadoop        303 2024-02-07 23:35 /user/root/HW2/NBmodel.txt\n","-rw-r--r--   1 root hadoop        119 2024-02-07 23:35 /user/root/HW2/chineseTest.txt\n","-rw-r--r--   1 root hadoop        107 2024-02-07 23:34 /user/root/HW2/chineseTrain.txt\n","drwxr-xr-x   - root hadoop          0 2024-02-07 04:03 /user/root/HW2/eda-output\n","drwxr-xr-x   - root hadoop          0 2024-02-07 04:37 /user/root/HW2/eda-sort-output\n","-rw-r--r--   1 root hadoop     204559 2024-02-07 02:08 /user/root/HW2/enron.txt\n"]}],"source":["!hdfs dfs -ls {HDFS_DIR}"]},{"cell_type":"markdown","metadata":{},"source":["Your work for `part b` starts here:"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["# part b - do your work in model/classify_mapper.py first, then run this cell.\n","!chmod a+x model/classify_mapper.py"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["d5  1  -8.90668134500626   -8.10769031284611   1\n","d6  1  -5.780743515794329  -4.179502370564408  1\n","d7  0  -6.591673732011658  -7.511706880737812  0\n","d8  0  -4.394449154674438  -5.565796731681498  0\n"]}],"source":["# part b - unit test model/classify_mapper.py (RUN THIS CELL AS IS)\n","!cat model/chineseTest.txt | model/classify_mapper.py | column -t"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted /user/root/HW2/chinese-output\n"]}],"source":["# part b - clear the output directory in HDFS (RUN THIS CELL AS IS)\n","!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"]},{"cell_type":"code","execution_count":134,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob7577554883876162334.jar tmpDir=null\n","24/02/08 01:54:11 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/08 01:54:12 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/08 01:54:12 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/08 01:54:12 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/08 01:54:12 INFO mapred.FileInputFormat: Total input files to process : 1\n","24/02/08 01:54:13 INFO mapreduce.JobSubmitter: number of splits:10\n","24/02/08 01:54:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707263943154_0013\n","24/02/08 01:54:13 INFO conf.Configuration: resource-types.xml not found\n","24/02/08 01:54:13 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/02/08 01:54:13 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/02/08 01:54:13 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/02/08 01:54:13 INFO impl.YarnClientImpl: Submitted application application_1707263943154_0013\n","24/02/08 01:54:13 INFO mapreduce.Job: The url to track the job: http://hw2hw2cluster-m:8088/proxy/application_1707263943154_0013/\n","24/02/08 01:54:13 INFO mapreduce.Job: Running job: job_1707263943154_0013\n","24/02/08 01:54:22 INFO mapreduce.Job: Job job_1707263943154_0013 running in uber mode : false\n","24/02/08 01:54:22 INFO mapreduce.Job:  map 0% reduce 0%\n","24/02/08 01:54:31 INFO mapreduce.Job:  map 10% reduce 0%\n","24/02/08 01:54:32 INFO mapreduce.Job:  map 30% reduce 0%\n","24/02/08 01:54:40 INFO mapreduce.Job:  map 60% reduce 0%\n","24/02/08 01:54:49 INFO mapreduce.Job:  map 90% reduce 0%\n","24/02/08 01:54:53 INFO mapreduce.Job:  map 100% reduce 0%\n","24/02/08 01:54:58 INFO mapreduce.Job:  map 100% reduce 100%\n","24/02/08 01:55:01 INFO mapreduce.Job: Job job_1707263943154_0013 completed successfully\n","24/02/08 01:55:01 INFO mapreduce.Job: Counters: 49\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=192\n","\t\tFILE: Number of bytes written=2459653\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1645\n","\t\tHDFS: Number of bytes written=178\n","\t\tHDFS: Number of read operations=35\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\tJob Counters \n","\t\tLaunched map tasks=10\n","\t\tLaunched reduce tasks=1\n","\t\tData-local map tasks=10\n","\t\tTotal time spent by all maps in occupied slots (ms)=221913\n","\t\tTotal time spent by all reduces in occupied slots (ms)=8928\n","\t\tTotal time spent by all map tasks (ms)=73971\n","\t\tTotal time spent by all reduce tasks (ms)=2976\n","\t\tTotal vcore-milliseconds taken by all map tasks=73971\n","\t\tTotal vcore-milliseconds taken by all reduce tasks=2976\n","\t\tTotal megabyte-milliseconds taken by all map tasks=227238912\n","\t\tTotal megabyte-milliseconds taken by all reduce tasks=9142272\n","\tMap-Reduce Framework\n","\t\tMap input records=4\n","\t\tMap output records=4\n","\t\tMap output bytes=178\n","\t\tMap output materialized bytes=246\n","\t\tInput split bytes=1040\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=4\n","\t\tReduce shuffle bytes=246\n","\t\tReduce input records=4\n","\t\tReduce output records=4\n","\t\tSpilled Records=8\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=1961\n","\t\tCPU time spent (ms)=24590\n","\t\tPhysical memory (bytes) snapshot=6148231168\n","\t\tVirtual memory (bytes) snapshot=48588902400\n","\t\tTotal committed heap usage (bytes)=5677514752\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=605\n","\tFile Output Format Counters \n","\t\tBytes Written=178\n","24/02/08 01:55:01 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-output\n"]}],"source":["# part c - write your Hadooop streaming job here\n","!hadoop jar {JAR_FILE} \\\n","  -files /model/classify_mapper.py,NBmodel.txt \\\n","  -mapper /model/classify_mapper.py \\\n","  -input {HDFS_DIR}/chineseTest.txt \\\n","  -output {HDFS_DIR}/chinese-output \\\n","  -numReduceTasks 1 \\\n","  -cmdenv PATH={PATH}"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["# part c - retrieve test set results from HDFS (RUN THIS CELL AS IS)\n","!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > model/chineseResults.txt"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["d5  1  -8.90668134500626   -8.10769031284611   1\n","d6  1  -5.780743515794329  -4.179502370564408  1\n","d7  0  -6.591673732011658  -7.511706880737812  0\n","d8  0  -4.394449154674438  -5.565796731681498  0\n"]}],"source":["# part c - take a look (RUN THIS CELL AS IS)\n","!cat model/chineseResults.txt | column -t"]},{"cell_type":"markdown","metadata":{},"source":["<table>\n","<th> Expected output for the test set for parts b) and c):</th>\n","<tr align=Left><td><pre>\n","d5\t1\t-8.90668134\t-8.10769031\t1\n","d6\t1\t-5.78074351\t-4.17950237\t1\n","d7\t0\t-6.59167373\t-7.51170688\t0\n","d8\t0\t-4.39444915\t-5.56579673\t0\n","</pre></td><tr>\n","</table>"]},{"cell_type":"markdown","metadata":{},"source":["# Question 4: Naive Bayes Training\n","Previously, you trained a model by hand. In this question, you'll develop the code to do the same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n","\n","To refresh your memory about the training process, review Q3 and write down the pieces of information needed to encode a Multinomial Naive Bayes model. We will retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n","\n","The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totaled up _before_ you start estimating the class conditional probabilities for each word. \n","\n","**[IMPORTANT NOTE:]** you only need to use 1 reducer for this question. \n","\n","\n","### Tasks:\n","* __a) make a plan:__  Read the docstrings (comments) in __`model/train_mapper.py`__ and fill in the docstrings for __`model/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`email.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n","\n","\n","* __b) implement mapper and reducer:__ Complete the code in __`model/train_mapper.py`__ and __`model/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Make sure your end result is formatted correctly (see note above). Test your scripts independently and together (using `chineseTrain.txt` or test input of your own choosing). When you are satisfied with your Python code design, run a Hadoop streaming command to run your job in parallel on the __chineseTrain.txt__. Confirm that your trained model matches your hand calculations from Question 6.\n","\n","\n","* __c) short answer:__ You saw in the previous question that adding Laplace smoothing (where the smoothing parameter $k=1$) makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` review the smoothing equation again].\n","\n","\n","* __d) implement a smoothing reducer:__ Complete the code in __`model/train_reducer_smooth.py`__ to implement the algorithm with LaPlace smoothing. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you above. \n","\n","    - [`HINT:` Don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications]. \n"]},{"cell_type":"markdown","metadata":{},"source":["### Q4 Student Answers:\n","\n","> __c)__ The number of unique words in the vocabulary.\n"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=========== REDUCER DOCSTRING ============\n","Reducer aggregates word counts by class and emits frequencies.\n","\n","INPUT:                                                     \n","    word \\t class0_partialCount,class1_partialCount  \n","OUTPUT:\n","    word \\t ham_count,spam_count,P(word|ham),P(word|spam)\n"]}],"source":["# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n","!chmod a+x model/train_mapper.py\n","!chmod a+x model/train_reducer.py\n","\n","!echo \"=========== REDUCER DOCSTRING ============\"\n","!head -n 8 model/train_reducer.py | tail -n 6"]},{"cell_type":"markdown","metadata":{},"source":["__`part b starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["#totals      15,5\n","ClassPriors  3,1\n","christmas    3,1\n","tree         3,1\n","farm         3,1\n","pictures     3,1\n","na           3,1\n"]}],"source":["# part b - write a unit test for your mapper here\n","!echo '0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t1\t christmas tree farm pictures\tNA'| /model/train_mapper.py | column -t"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["christmas    8,1,1.0,0.5\n","tree         4,1,0.5,0.5\n","ClassPriors  4,2,0.6666666666666666,0.3333333333333333\n"]}],"source":["# part b - write a unit test for your reducer here\n","!echo '#totals\t4,1\\n#totals\t4,1\\nClassPriors\t2,1\\nClassPriors\t2,1\\nchristmas\t4,1\\nchristmas\t4,0\\ntree\t2,0\\ntree\t2,1'| /model/train_reducer.py |column -t"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["christmas    3,1,0.2,0.2\n","tree         3,1,0.2,0.2\n","farm         3,1,0.2,0.2\n","pictures     3,1,0.2,0.2\n","na           3,1,0.2,0.2\n","ClassPriors  3,1,0.75,0.25\n"]}],"source":["# part b - write a systems test for your mapper + reducer together here\n","!echo '0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t1\t christmas tree farm pictures\tNA' | /model/train_mapper.py | /model/train_reducer.py |  column -t"]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7 items\n","-rw-r--r--   1 root hadoop        303 2024-02-07 23:35 /user/root/HW2/NBmodel.txt\n","drwxr-xr-x   - root hadoop          0 2024-02-08 01:54 /user/root/HW2/chinese-output\n","-rw-r--r--   1 root hadoop        119 2024-02-07 23:35 /user/root/HW2/chineseTest.txt\n","-rw-r--r--   1 root hadoop        107 2024-02-07 23:34 /user/root/HW2/chineseTrain.txt\n","drwxr-xr-x   - root hadoop          0 2024-02-07 04:03 /user/root/HW2/eda-output\n","drwxr-xr-x   - root hadoop          0 2024-02-07 04:37 /user/root/HW2/eda-sort-output\n","-rw-r--r--   1 root hadoop     204559 2024-02-07 02:08 /user/root/HW2/enron.txt\n"]}],"source":["!hdfs dfs -ls {HDFS_DIR}"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted /user/root/HW2/unsmoothed-chinese-output\n"]}],"source":["# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model\n","!hdfs dfs -rm -r {HDFS_DIR}/unsmoothed-chinese-output"]},{"cell_type":"code","execution_count":201,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob5267554845565022786.jar tmpDir=null\n","24/02/09 00:46:54 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/09 00:46:54 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/09 00:46:54 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/09 00:46:54 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/09 00:46:55 INFO mapred.FileInputFormat: Total input files to process : 1\n","24/02/09 00:46:55 INFO mapreduce.JobSubmitter: number of splits:10\n","24/02/09 00:46:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707263943154_0015\n","24/02/09 00:46:56 INFO conf.Configuration: resource-types.xml not found\n","24/02/09 00:46:56 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/02/09 00:46:56 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/02/09 00:46:56 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/02/09 00:46:56 INFO impl.YarnClientImpl: Submitted application application_1707263943154_0015\n","24/02/09 00:46:56 INFO mapreduce.Job: The url to track the job: http://hw2hw2cluster-m:8088/proxy/application_1707263943154_0015/\n","24/02/09 00:46:56 INFO mapreduce.Job: Running job: job_1707263943154_0015\n","24/02/09 00:47:04 INFO mapreduce.Job: Job job_1707263943154_0015 running in uber mode : false\n","24/02/09 00:47:04 INFO mapreduce.Job:  map 0% reduce 0%\n","24/02/09 00:47:13 INFO mapreduce.Job:  map 10% reduce 0%\n","24/02/09 00:47:14 INFO mapreduce.Job:  map 30% reduce 0%\n","24/02/09 00:47:22 INFO mapreduce.Job:  map 60% reduce 0%\n","24/02/09 00:47:29 INFO mapreduce.Job:  map 70% reduce 0%\n","24/02/09 00:47:31 INFO mapreduce.Job:  map 90% reduce 0%\n","24/02/09 00:47:34 INFO mapreduce.Job:  map 100% reduce 0%\n","24/02/09 00:47:40 INFO mapreduce.Job:  map 100% reduce 100%\n","24/02/09 00:47:42 INFO mapreduce.Job: Job job_1707263943154_0015 completed successfully\n","24/02/09 00:47:42 INFO mapreduce.Job: Counters: 50\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=447\n","\t\tFILE: Number of bytes written=2468490\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1625\n","\t\tHDFS: Number of bytes written=194\n","\t\tHDFS: Number of read operations=35\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\tJob Counters \n","\t\tKilled map tasks=1\n","\t\tLaunched map tasks=10\n","\t\tLaunched reduce tasks=1\n","\t\tData-local map tasks=10\n","\t\tTotal time spent by all maps in occupied slots (ms)=215616\n","\t\tTotal time spent by all reduces in occupied slots (ms)=9312\n","\t\tTotal time spent by all map tasks (ms)=71872\n","\t\tTotal time spent by all reduce tasks (ms)=3104\n","\t\tTotal vcore-milliseconds taken by all map tasks=71872\n","\t\tTotal vcore-milliseconds taken by all reduce tasks=3104\n","\t\tTotal megabyte-milliseconds taken by all map tasks=220790784\n","\t\tTotal megabyte-milliseconds taken by all reduce tasks=9535488\n","\tMap-Reduce Framework\n","\t\tMap input records=4\n","\t\tMap output records=29\n","\t\tMap output bytes=383\n","\t\tMap output materialized bytes=501\n","\t\tInput split bytes=1050\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=8\n","\t\tReduce shuffle bytes=501\n","\t\tReduce input records=29\n","\t\tReduce output records=7\n","\t\tSpilled Records=58\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=1796\n","\t\tCPU time spent (ms)=22820\n","\t\tPhysical memory (bytes) snapshot=5889163264\n","\t\tVirtual memory (bytes) snapshot=48675786752\n","\t\tTotal committed heap usage (bytes)=5165809664\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=575\n","\tFile Output Format Counters \n","\t\tBytes Written=194\n","24/02/09 00:47:42 INFO streaming.StreamJob: Output directory: /user/root/HW2/unsmoothed-chinese-output\n"]}],"source":["# part b - write your hadoop streaming job\n","!hadoop jar {JAR_FILE} \\\n","  -files /model/train_mapper.py,/model/train_reducer.py \\\n","  -mapper /model/train_mapper.py \\\n","  -reducer /model/train_reducer.py \\\n","  -input {HDFS_DIR}/chineseTrain.txt \\\n","  -output {HDFS_DIR}/unsmoothed-chinese-output \\\n","  -numReduceTasks 1 \\\n","  -cmdenv PATH={PATH}"]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[],"source":["# part b - extract your results (i.e. model) to a local file\n","!hdfs dfs -cat {HDFS_DIR}/unsmoothed-chinese-output/part-0000* > /unsmoothed-chinese-results.txt"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["beijing      0,1,0.0,0.125\n","chinese      1,5,0.3333333333333333,0.625\n","japan        1,0,0.3333333333333333,0.0\n","macao        0,1,0.0,0.125\n","shanghai     0,1,0.0,0.125\n","tokyo        1,0,0.3333333333333333,0.0\n","ClassPriors  1,3,0.25,0.75\n"]}],"source":["# part b - print your model so that we can confirm that it matches expected results\n","!cat /unsmoothed-chinese-results.txt | column -t"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["D1  1  Chinese  Beijing  Chinese\n","D2  1  Chinese  Chinese  Shanghai\n","D3  1  Chinese  Macao\n","D4  0  Tokyo    Japan    Chinese\n"]}],"source":["!cat model/chineseTrain.txt | column -t"]},{"cell_type":"markdown","metadata":{},"source":["__`part d starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":["!chmod a+x /model/train_reducer_smooth.py"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["christmas    8,1,0.643,0.25\n","tree         4,1,0.357,0.25\n","ClassPriors  4,2,0.667,0.333\n"]}],"source":["# part d - write a unit test for your NEW reducer here\n","!echo '#totals\t4,1\\n#totals\t4,1\\nClassPriors\t2,1\\nClassPriors\t2,1\\nchristmas\t4,1\\nchristmas\t4,0\\ntree\t2,0\\ntree\t2,1'| /model/train_reducer_smooth.py |column -t"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["christmas    3,1,0.19,0.182\n","tree         3,1,0.19,0.182\n","farm         3,1,0.19,0.182\n","pictures     3,1,0.19,0.182\n","na           3,1,0.19,0.182\n","ClassPriors  3,1,0.75,0.25\n"]}],"source":["# part d - write a systems test for your mapper + reducer together here\n","!echo '0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t0\t christmas tree farm pictures\tNA\\n0001\t1\t christmas tree farm pictures\tNA' | /model/train_mapper.py | /model/train_reducer_smooth.py |  column -t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part d - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n","!hdfs dfs -rm -r {HDFS_DIR}/smoothed-chinese-output"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.10.2.jar] /tmp/streamjob4877839433064442159.jar tmpDir=null\n","24/02/09 01:02:44 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/09 01:02:44 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/09 01:02:44 INFO client.RMProxy: Connecting to ResourceManager at hw2hw2cluster-m/10.128.0.2:8032\n","24/02/09 01:02:44 INFO client.AHSProxy: Connecting to Application History server at hw2hw2cluster-m/10.128.0.2:10200\n","24/02/09 01:02:45 INFO mapred.FileInputFormat: Total input files to process : 1\n","24/02/09 01:02:45 INFO mapreduce.JobSubmitter: number of splits:10\n","24/02/09 01:02:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707263943154_0016\n","24/02/09 01:02:46 INFO conf.Configuration: resource-types.xml not found\n","24/02/09 01:02:46 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/02/09 01:02:46 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/02/09 01:02:46 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/02/09 01:02:46 INFO impl.YarnClientImpl: Submitted application application_1707263943154_0016\n","24/02/09 01:02:46 INFO mapreduce.Job: The url to track the job: http://hw2hw2cluster-m:8088/proxy/application_1707263943154_0016/\n","24/02/09 01:02:46 INFO mapreduce.Job: Running job: job_1707263943154_0016\n","24/02/09 01:02:54 INFO mapreduce.Job: Job job_1707263943154_0016 running in uber mode : false\n","24/02/09 01:02:54 INFO mapreduce.Job:  map 0% reduce 0%\n","24/02/09 01:03:04 INFO mapreduce.Job:  map 30% reduce 0%\n","24/02/09 01:03:12 INFO mapreduce.Job:  map 50% reduce 0%\n","24/02/09 01:03:13 INFO mapreduce.Job:  map 60% reduce 0%\n","24/02/09 01:03:19 INFO mapreduce.Job:  map 70% reduce 0%\n","24/02/09 01:03:21 INFO mapreduce.Job:  map 90% reduce 0%\n","24/02/09 01:03:24 INFO mapreduce.Job:  map 100% reduce 0%\n","24/02/09 01:03:30 INFO mapreduce.Job:  map 100% reduce 100%\n","24/02/09 01:03:32 INFO mapreduce.Job: Job job_1707263943154_0016 completed successfully\n","24/02/09 01:03:32 INFO mapreduce.Job: Counters: 49\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=447\n","\t\tFILE: Number of bytes written=2468776\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1625\n","\t\tHDFS: Number of bytes written=165\n","\t\tHDFS: Number of read operations=35\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\tJob Counters \n","\t\tLaunched map tasks=10\n","\t\tLaunched reduce tasks=1\n","\t\tData-local map tasks=10\n","\t\tTotal time spent by all maps in occupied slots (ms)=215061\n","\t\tTotal time spent by all reduces in occupied slots (ms)=9822\n","\t\tTotal time spent by all map tasks (ms)=71687\n","\t\tTotal time spent by all reduce tasks (ms)=3274\n","\t\tTotal vcore-milliseconds taken by all map tasks=71687\n","\t\tTotal vcore-milliseconds taken by all reduce tasks=3274\n","\t\tTotal megabyte-milliseconds taken by all map tasks=220222464\n","\t\tTotal megabyte-milliseconds taken by all reduce tasks=10057728\n","\tMap-Reduce Framework\n","\t\tMap input records=4\n","\t\tMap output records=29\n","\t\tMap output bytes=383\n","\t\tMap output materialized bytes=501\n","\t\tInput split bytes=1050\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=8\n","\t\tReduce shuffle bytes=501\n","\t\tReduce input records=29\n","\t\tReduce output records=7\n","\t\tSpilled Records=58\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=1744\n","\t\tCPU time spent (ms)=23230\n","\t\tPhysical memory (bytes) snapshot=6046445568\n","\t\tVirtual memory (bytes) snapshot=48708304896\n","\t\tTotal committed heap usage (bytes)=5488771072\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=575\n","\tFile Output Format Counters \n","\t\tBytes Written=165\n","24/02/09 01:03:32 INFO streaming.StreamJob: Output directory: /user/root/HW2/smoothed-chinese-output\n"]}],"source":["# part d - write your hadoop streaming job\n","!hadoop jar {JAR_FILE} \\\n","  -files /model/train_mapper.py,/model/train_reducer_smooth.py \\\n","  -mapper /model/train_mapper.py \\\n","  -reducer /model/train_reducer_smooth.py \\\n","  -input {HDFS_DIR}/chineseTrain.txt \\\n","  -output {HDFS_DIR}/smoothed-chinese-output \\\n","  -numReduceTasks 1 \\\n","  -cmdenv PATH={PATH}"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[],"source":["# part d - extract your results (i.e. model) to a local file\n","!hdfs dfs -cat {HDFS_DIR}/smoothed-chinese-output/part-0000* > /smoothed-chinese-results.txt"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["beijing      0,1,0.111,0.143\n","chinese      1,5,0.222,0.429\n","japan        1,0,0.222,0.071\n","macao        0,1,0.111,0.143\n","shanghai     0,1,0.111,0.143\n","tokyo        1,0,0.222,0.071\n","ClassPriors  1,3,0.25,0.75\n"]}],"source":["!cat /smoothed-chinese-results.txt | column -t"]},{"cell_type":"markdown","metadata":{},"source":["### Congratulations, you have completed HW2! \n","\n","### Submission Instructions\n","You will need to submit a zip file to **Gradescope** containing the following files:\n","- This notebook HW2.ipynb\n","- From Question 1, eda/mapper.py, eda/reducer.py, and optionally mapper-by-class.py\n","- From Questions 3-4, model/classify_mapper.py, model/train_mapper.py, model/train_reducer.py, model/train_reducer_smooth.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}
